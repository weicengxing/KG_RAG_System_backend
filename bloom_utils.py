"""
å¸ƒéš†è¿‡æ»¤å™¨å·¥å…·æ¨¡å—
æä¾›åŸºäº pybloom çš„å¸ƒéš†è¿‡æ»¤å™¨å®ç°
æ”¯æŒå¯åŠ¨æ—¶Warmupé¢„åŠ è½½
"""

import pickle
import logging
import time
import threading
from typing import Optional, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from pybloom_live import ScalableBloomFilter, BloomFilter

from redis_utils import redis_client, redis_binary_client

logger = logging.getLogger(__name__)

# ==================== Warmup çŠ¶æ€ç®¡ç† ====================

BLOOM_WARMUP_COMPLETED = False  # Warmupå®Œæˆæ ‡å¿—
BLOOM_WARMUP_LOCK = threading.Lock()  # çº¿ç¨‹å®‰å…¨é”


# ==================== Redis å­˜å‚¨çš„å¸ƒéš†è¿‡æ»¤å™¨ ====================

class RedisBloomFilter:
    """åŸºäº Redis å­˜å‚¨çš„å¸ƒéš†è¿‡æ»¤å™¨ï¼ˆæŒä¹…åŒ–ï¼‰"""
    
    def __init__(
        self,
        key_name: str,
        initial_capacity: int = 100000,
        error_rate: float = 0.001,
        use_scalable: bool = True,
        batch_size: int = 100
    ):
        """
        åˆå§‹åŒ–å¸ƒéš†è¿‡æ»¤å™¨
        
        Args:
            key_name: Redis é”®å
            initial_capacity: åˆå§‹å®¹é‡
            error_rate: è¯¯åˆ¤ç‡ (0-1ä¹‹é—´ï¼Œè¶Šå°è¶Šç²¾å‡†)
            use_scalable: æ˜¯å¦ä½¿ç”¨å¯æ‰©å±•çš„å¸ƒéš†è¿‡æ»¤å™¨
            batch_size: æ‰¹é‡ä¿å­˜çš„é˜ˆå€¼ï¼ˆè¾¾åˆ°æ­¤æ¬¡æ•°æ‰ä¿å­˜åˆ° Redisï¼‰
        """
        self.redis = redis_binary_client
        self.key_name = key_name
        self.initial_capacity = initial_capacity
        self.error_rate = error_rate
        self.use_scalable = use_scalable
        self.batch_size = batch_size
        self.bloom: Optional[ScalableBloomFilter] = None
        self._pending_count = 0  # å¾…ä¿å­˜çš„è®¡æ•°å™¨
        
        self._load_bloom()
    
    def _load_bloom(self):
        """ä» Redis åŠ è½½å¸ƒéš†è¿‡æ»¤å™¨"""
        try:
            data = self.redis.get(self.key_name)
            if data:
                # ç¡®ä¿æ•°æ®æ˜¯bytesç±»å‹ï¼ˆpickleéœ€è¦bytesï¼Œä¸èƒ½æ˜¯strï¼‰
                if isinstance(data, str):
                    logger.warning(f"âš ï¸ Redisè¿”å›äº†å­—ç¬¦ä¸²ç±»å‹ï¼Œå°è¯•ç¼–ç ä¸ºbytes: {self.key_name}")
                    try:
                        data = data.encode('utf-8')
                    except Exception as encode_error:
                        logger.error(f"âŒ å­—ç¬¦ä¸²ç¼–ç å¤±è´¥: {encode_error}")
                        # ç¼–ç å¤±è´¥ï¼Œæ¸…é™¤æŸåçš„æ•°æ®å¹¶é‡æ–°åˆå§‹åŒ–
                        self.redis.delete(self.key_name)
                        data = None
                
                if data:
                    self.bloom = pickle.loads(data)
                    logger.info(f"âœ… ä» Redis åŠ è½½å¸ƒéš†è¿‡æ»¤å™¨: {self.key_name}")
                else:
                    logger.warning(f"âš ï¸ Redisä¸­çš„å¸ƒéš†è¿‡æ»¤å™¨æ•°æ®æŸåï¼Œé‡æ–°åˆå§‹åŒ–: {self.key_name}")
                    if self.use_scalable:
                        self.bloom = ScalableBloomFilter(
                            initial_capacity=self.initial_capacity,
                            error_rate=self.error_rate
                        )
                    else:
                        self.bloom = BloomFilter(
                            capacity=self.initial_capacity,
                            error_rate=self.error_rate
                        )
            else:
                if self.use_scalable:
                    self.bloom = ScalableBloomFilter(
                        initial_capacity=self.initial_capacity,
                        error_rate=self.error_rate
                    )
                else:
                    self.bloom = BloomFilter(
                        capacity=self.initial_capacity,
                        error_rate=self.error_rate
                    )
                logger.info(f"âœ… åˆ›å»ºæ–°å¸ƒéš†è¿‡æ»¤å™¨: {self.key_name}")
        except pickle.UnpicklingError as e:
            logger.error(f"âŒ Pickleååºåˆ—åŒ–å¤±è´¥: {e}")
            logger.error(f"ğŸ—‘ï¸ æ¸…é™¤æŸåçš„å¸ƒéš†è¿‡æ»¤å™¨æ•°æ®: {self.key_name}")
            self.redis.delete(self.key_name)
            # é‡æ–°åˆå§‹åŒ–
            if self.use_scalable:
                self.bloom = ScalableBloomFilter(
                    initial_capacity=self.initial_capacity,
                    error_rate=self.error_rate
                )
            else:
                self.bloom = BloomFilter(
                    capacity=self.initial_capacity,
                    error_rate=self.error_rate
                )
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¸ƒéš†è¿‡æ»¤å™¨å¤±è´¥: {e}")
            logger.error(f"ğŸ—‘ï¸ æ¸…é™¤æŸåçš„å¸ƒéš†è¿‡æ»¤å™¨æ•°æ®: {self.key_name}")
            self.redis.delete(self.key_name)
            self.bloom = ScalableBloomFilter(
                initial_capacity=self.initial_capacity,
                error_rate=self.error_rate
            )
    
    def _save_bloom(self):
        """ä¿å­˜å¸ƒéš†è¿‡æ»¤å™¨åˆ° Redis"""
        try:
            if self.bloom:
                data = pickle.dumps(self.bloom)
                # ä¿å­˜7å¤©ï¼Œè¿‡æœŸåè‡ªåŠ¨é‡å»º
                self.redis.setex(self.key_name, 7 * 24 * 3600, data)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¸ƒéš†è¿‡æ»¤å™¨å¤±è´¥: {e}")
    
    def add(self, item: str, force_save: bool = False) -> bool:
        """
        æ·»åŠ å…ƒç´ 
        
        Args:
            item: è¦æ·»åŠ çš„å…ƒç´ 
            force_save: æ˜¯å¦å¼ºåˆ¶ä¿å­˜åˆ° Redisï¼ˆé»˜è®¤ä½¿ç”¨æ‰¹é‡ä¿å­˜ï¼‰
        
        Returns:
            bool: True è¡¨ç¤ºæ·»åŠ æˆåŠŸï¼ŒFalse è¡¨ç¤ºå·²å­˜åœ¨
        """
        if not self.bloom:
            self._load_bloom()
        
        item = str(item)
        result = item not in self.bloom
        self.bloom.add(item)
        
        # æ‰¹é‡ä¿å­˜ç­–ç•¥
        self._pending_count += 1
        if force_save or self._pending_count >= self.batch_size:
            self._save_bloom()
            self._pending_count = 0
        
        return result
    
    def exists(self, item: str) -> bool:
        """
        æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨
        
        Args:
            item: è¦æ£€æŸ¥çš„å…ƒç´ 
        
        Returns:
            bool: å¯èƒ½å­˜åœ¨ï¼ˆæ³¨æ„ï¼šå¸ƒéš†è¿‡æ»¤å™¨æœ‰è¯¯åˆ¤ï¼‰
        """
        if not self.bloom:
            self._load_bloom()
        
        item = str(item)
        return item in self.bloom
    
    def multi_add(self, items: List[str]) -> List[bool]:
        """æ‰¹é‡æ·»åŠ å…ƒç´ ï¼ˆä¼˜åŒ–ç‰ˆï¼Œåªä¿å­˜ä¸€æ¬¡ï¼‰"""
        if not self.bloom:
            self._load_bloom()
        
        results = []
        for item in items:
            item = str(item)
            result = item not in self.bloom
            self.bloom.add(item)
            results.append(result)
        
        # æ‰¹é‡ä¿å­˜ä¸€æ¬¡
        self._pending_count += len(items)
        if self._pending_count >= self.batch_size:
            self._save_bloom()
            self._pending_count = 0
        
        return results
    
    def multi_exists(self, items: List[str]) -> List[bool]:
        """æ‰¹é‡æ£€æŸ¥å…ƒç´ """
        if not self.bloom:
            self._load_bloom()
        
        return [str(item) in self.bloom for item in items]
    
    def clear(self):
        """æ¸…ç©ºå¸ƒéš†è¿‡æ»¤å™¨"""
        self.redis.delete(self.key_name)
        self._pending_count = 0  # é‡ç½®è®¡æ•°å™¨
        if self.use_scalable:
            self.bloom = ScalableBloomFilter(
                initial_capacity=self.initial_capacity,
                error_rate=self.error_rate
            )
        else:
            self.bloom = BloomFilter(
                capacity=self.initial_capacity,
                error_rate=self.error_rate
            )
        logger.info(f"ğŸ—‘ï¸ å¸ƒéš†è¿‡æ»¤å™¨å·²æ¸…ç©º: {self.key_name}")


# ==================== å…¨å±€å¸ƒéš†è¿‡æ»¤å™¨å®ä¾‹ ====================

# 1. æ–‡æ¡£ä¸Šä¼ å»é‡å¸ƒéš†è¿‡æ»¤å™¨
# ä½¿ç”¨è¾ƒå°çš„ batch_sizeï¼Œå› ä¸ºæ–‡æ¡£ä¸Šä¼ ç›¸å¯¹ä½é¢‘ï¼Œæ¯æ¬¡éƒ½å¼ºåˆ¶ä¿å­˜æ›´å®‰å…¨
document_bloom = RedisBloomFilter(
    key_name="bloom:uploaded_documents",
    initial_capacity=1000000,  # é¢„ä¼°100ä¸‡æ–‡æ¡£
    error_rate=0.001,          # 0.1% è¯¯åˆ¤ç‡
    batch_size=10              # æ¯10æ¬¡æ“ä½œä¿å­˜ä¸€æ¬¡ï¼Œæˆ–ä½¿ç”¨ force_save=True å¼ºåˆ¶ä¿å­˜
)

# 2. ç¼“å­˜ç©¿é€é˜²æŠ¤å¸ƒéš†è¿‡æ»¤å™¨
# ä½¿ç”¨è¾ƒå¤§çš„ batch_sizeï¼Œå› ä¸ºé«˜é¢‘æ“ä½œï¼Œå‡å°‘ Redis å†™å…¥å‹åŠ›
cache_keys_bloom = RedisBloomFilter(
    key_name="bloom:cache_keys",
    initial_capacity=10000000,  # é¢„ä¼°1000ä¸‡ç¼“å­˜é”®
    error_rate=0.0001,          # 0.01% è¯¯åˆ¤ç‡
    batch_size=1000             # æ¯1000æ¬¡æ“ä½œä¿å­˜ä¸€æ¬¡
)


# ==================== ä¾¿æ·å‡½æ•° ====================

def is_document_uploaded(doc_id: str) -> bool:
    """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ä¸Šä¼ """
    return document_bloom.exists(doc_id)

def mark_document_uploaded(doc_id: str, force_save: bool = False):
    """
    æ ‡è®°æ–‡æ¡£å·²ä¸Šä¼ 
    Args:
        doc_id: æ–‡æ¡£IDï¼ˆMD5 hashï¼‰
        force_save: æ˜¯å¦å¼ºåˆ¶ç«‹å³ä¿å­˜åˆ° Redisï¼ˆé»˜è®¤ä½¿ç”¨æ‰¹é‡ä¿å­˜ï¼‰
    """
    document_bloom.add(doc_id, force_save=force_save)

def is_cache_key_exists(key: str) -> bool:
    """æ£€æŸ¥ç¼“å­˜é”®æ˜¯å¦å­˜åœ¨"""
    return cache_keys_bloom.exists(key)

def mark_cache_key_exists(key: str, force_save: bool = False):
    """
    æ ‡è®°ç¼“å­˜é”®å·²å­˜åœ¨
    Args:
        key: ç¼“å­˜é”®å
        force_save: æ˜¯å¦å¼ºåˆ¶ç«‹å³ä¿å­˜åˆ° Redisï¼ˆé»˜è®¤ä½¿ç”¨æ‰¹é‡ä¿å­˜ï¼‰
    """
    cache_keys_bloom.add(key, force_save=force_save)


# ==================== Bloom Filter Warmup åŠŸèƒ½ ====================

def warmup_document_bloom_from_mongodb():
    """
    ä»MongoDBé¢„çƒ­document_bloom (æ–‡æ¡£å»é‡å¸ƒéš†è¿‡æ»¤å™¨)
    è¯»å–æ‰€æœ‰æ–‡æ¡£çš„file_hashå¹¶æ·»åŠ åˆ°å¸ƒéš†è¿‡æ»¤å™¨
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    global BLOOM_WARMUP_COMPLETED
    
    logger.info("ğŸš€ å¼€å§‹é¢„çƒ­ document_bloom (ä»MongoDB)...")
    start_time = time.time()
    
    try:
        # ä½¿ç”¨åŒæ­¥çš„pymongoå®¢æˆ·ç«¯ï¼ˆå› ä¸ºæ˜¯åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
        from pymongo import MongoClient
        from config import MONGO_URI, MONGO_DB_NAME
        
        # è¿æ¥MongoDB
        client = MongoClient(MONGO_URI)
        db = client[MONGO_DB_NAME]
        
        # æŸ¥è¯¢æ‰€æœ‰æ–‡æ¡£çš„file_hash
        total_count = 0
        added_count = 0
        
        # ä½¿ç”¨æ¸¸æ ‡æ‰¹é‡è¯»å–ï¼Œé¿å…å†…å­˜æº¢å‡º
        cursor = db.documents.find({}, {"file_hash": 1, "_id": 0})
        
        for doc in cursor:
            total_count += 1
            file_hash = doc.get("file_hash")
            if file_hash:
                document_bloom.add(file_hash, force_save=False)
                added_count += 1
            
            # æ¯1000æ¡è®°å½•ä¸€æ¬¡è¿›åº¦æ—¥å¿—
            if total_count % 1000 == 0:
                logger.info(f"ğŸ“¦ å·²å¤„ç† {total_count} æ¡æ–‡æ¡£è®°å½•...")
        
        # Warmupå®Œæˆåä¿å­˜åˆ°Redis
        document_bloom._save_bloom()
        
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… document_bloomé¢„çƒ­å®Œæˆï¼")
        logger.info(f"   - æ€»æ–‡æ¡£æ•°: {total_count}")
        logger.info(f"   - æˆåŠŸæ·»åŠ : {added_count}")
        logger.info(f"   - è€—æ—¶: {elapsed_time:.2f}ç§’")
        
        client.close()
        
        return {
            "success": True,
            "total_count": total_count,
            "added_count": added_count,
            "elapsed_time": elapsed_time
        }
        
    except Exception as e:
        logger.error(f"âŒ document_bloomé¢„çƒ­å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


def warmup_cache_keys_bloom_from_neo4j():
    """
    ä»Neo4jé¢„çƒ­cache_keys_bloom (ç¼“å­˜é”®å¸ƒéš†è¿‡æ»¤å™¨)
    æ‰«ææ‰€æœ‰ç”¨æˆ·çš„qr_login_statuså¹¶ç”Ÿæˆç¼“å­˜é”®
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    global BLOOM_WARMUP_COMPLETED
    
    logger.info("ğŸš€ å¼€å§‹é¢„çƒ­ cache_keys_bloom (ä»Neo4j)...")
    start_time = time.time()
    
    try:
        from database import driver
        
        if not driver:
            logger.warning("âš ï¸ Neo4jè¿æ¥æœªå»ºç«‹ï¼Œè·³è¿‡cache_keys_bloomé¢„çƒ­")
            return {
                "success": True,
                "skipped": True,
                "reason": "Neo4j connection not available"
            }
        
        total_count = 0
        added_count = 0
        
        with driver.session() as session:
            query = """
            MATCH (u:User)
            WHERE u.qr_login_enabled IS NOT NULL
            RETURN u.username as username
            """
            results = session.run(query)
            
            for record in results:
                total_count += 1
                username = record.get("username")
                if username:
                    cache_key = f"qr_login_status:{username}"
                    cache_keys_bloom.add(cache_key, force_save=False)
                    added_count += 1
                
                # æ¯100æ¡è®°å½•ä¸€æ¬¡è¿›åº¦æ—¥å¿—
                if total_count % 100 == 0:
                    logger.info(f"ğŸ“¦ å·²å¤„ç† {total_count} ä¸ªç”¨æˆ·...")
        
        # Warmupå®Œæˆåä¿å­˜åˆ°Redis
        cache_keys_bloom._save_bloom()
        
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… cache_keys_bloomé¢„çƒ­å®Œæˆï¼")
        logger.info(f"   - æ€»ç”¨æˆ·æ•°: {total_count}")
        logger.info(f"   - æˆåŠŸæ·»åŠ : {added_count}")
        logger.info(f"   - è€—æ—¶: {elapsed_time:.2f}ç§’")
        
        return {
            "success": True,
            "total_count": total_count,
            "added_count": added_count,
            "elapsed_time": elapsed_time
        }
        
    except Exception as e:
        logger.error(f"âŒ cache_keys_bloomé¢„çƒ­å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


def warmup_all_bloom_filters_async():
    """
    å¼‚æ­¥é¢„çƒ­æ‰€æœ‰å¸ƒéš†è¿‡æ»¤å™¨ï¼ˆåœ¨åå°çº¿ç¨‹æ± ä¸­å¹¶å‘æ‰§è¡Œï¼‰
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼Œä½¿ç”¨çº¿ç¨‹æ± é¿å…é˜»å¡å¯åŠ¨
    ä½¿ç”¨ concurrent.futures å®ç°å¹¶å‘é¢„çƒ­ï¼Œæé«˜çƒ­èº«æ•ˆç‡
    """
    global BLOOM_WARMUP_COMPLETED, BLOOM_WARMUP_LOCK
    
    def _warmup_task():
        """Warmupä»»åŠ¡çš„å†…éƒ¨å‡½æ•°ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰"""
        logger.info("=" * 60)
        logger.info("ğŸ”¥ Bloom Filter Warmup å¼€å§‹ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰")
        logger.info("=" * 60)
        
        # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘æ‰§è¡Œä¸¤ä¸ªé¢„çƒ­ä»»åŠ¡
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=2, thread_name_prefix="bloom-warmup") as executor:
            # æäº¤ä¸¤ä¸ªé¢„çƒ­ä»»åŠ¡
            future_doc = executor.submit(warmup_document_bloom_from_mongodb)
            future_cache = executor.submit(warmup_cache_keys_bloom_from_neo4j)
            
            # æŒ‰å®Œæˆé¡ºåºè·å–ç»“æœ
            results = {}
            for future in as_completed([future_doc, future_cache]):
                try:
                    result = future.result()
                    # æ ¹æ®ä»»åŠ¡ç±»å‹æ ‡è®°ç»“æœ
                    if 'file_hash' in str(result.get('error', '')) or 'document' in str(result):
                        results['document'] = result
                    else:
                        results['cache'] = result
                except Exception as e:
                    logger.error(f"âŒ Warmupä»»åŠ¡å¼‚å¸¸: {e}")
                    # æ ‡è®°å¤±è´¥çš„ç»“æœ
                    if future == future_doc:
                        results['document'] = {'success': False, 'error': str(e)}
                    else:
                        results['cache'] = {'success': False, 'error': str(e)}
            
            # ç¡®ä¿ä¸¤ä¸ªç»“æœéƒ½å­˜åœ¨ï¼ˆæŒ‰æäº¤é¡ºåºï¼‰
            if 'document' not in results:
                results['document'] = future_doc.result()
            if 'cache' not in results:
                results['cache'] = future_cache.result()
        
        elapsed_time = time.time() - start_time
        
        # æ ‡è®°Warmupå®Œæˆ
        with BLOOM_WARMUP_LOCK:
            BLOOM_WARMUP_COMPLETED = True
        
        # è¾“å‡ºç»“æœ
        doc_result = results.get('document', {'success': False})
        cache_result = results.get('cache', {'success': False})
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ Bloom Filter Warmup å…¨éƒ¨å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š æ–‡æ¡£å¸ƒéš†è¿‡æ»¤å™¨: {'âœ…' if doc_result.get('success') else 'âŒ'}")
        if doc_result.get('success'):
            logger.info(f"   - å¤„ç†æ–‡æ¡£æ•°: {doc_result.get('total_count', 0)}")
        logger.info(f"ğŸ“Š ç¼“å­˜å¸ƒéš†è¿‡æ»¤å™¨: {'âœ…' if cache_result.get('success') else 'âŒ'}")
        if cache_result.get('success'):
            logger.info(f"   - å¤„ç†ç”¨æˆ·æ•°: {cache_result.get('total_count', 0)}")
        logger.info(f"âš¡ æ€»è€—æ—¶: {elapsed_time:.2f}ç§’ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰")
        logger.info("ğŸ”„ WarmupæœŸé—´çš„ä¸šåŠ¡è¯·æ±‚å·²è‡ªåŠ¨åˆ‡æ¢åˆ°å¸ƒéš†è¿‡æ»¤å™¨æ£€æŸ¥")
        logger.info("=" * 60)
    
    # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡ŒWarmup
    warmup_thread = threading.Thread(target=_warmup_task, daemon=True)
    warmup_thread.start()
    
    logger.info("ğŸ“¢ Bloom Filter Warmup å·²åœ¨åå°å¯åŠ¨ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰...")
    logger.info("âš ï¸  WarmupæœŸé—´çš„ä¸¤ä¸ªä»»åŠ¡å°†å¹¶å‘æ‰§è¡Œï¼Œäº’ä¸é˜»å¡")


def is_warmup_completed() -> bool:
    """
    æ£€æŸ¥Warmupæ˜¯å¦å®Œæˆ
    
    Returns:
        bool: Trueè¡¨ç¤ºWarmupå·²å®Œæˆï¼ŒFalseè¡¨ç¤ºæ­£åœ¨è¿›è¡Œ
    """
    return BLOOM_WARMUP_COMPLETED


def wait_for_warmup_completion(timeout: Optional[float] = None) -> bool:
    """
    ç­‰å¾…Warmupå®Œæˆï¼ˆé˜»å¡è°ƒç”¨ï¼Œä¸€èˆ¬ç”¨äºæµ‹è¯•ï¼‰
    
    Args:
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸è¶…æ—¶
    
    Returns:
        bool: Trueè¡¨ç¤ºWarmupå®Œæˆï¼ŒFalseè¡¨ç¤ºè¶…æ—¶
    """
    global BLOOM_WARMUP_COMPLETED
    
    if BLOOM_WARMUP_COMPLETED:
        return True
    
    # ç®€å•è½®è¯¢æ£€æŸ¥
    import time
    start_time = time.time()
    
    while not BLOOM_WARMUP_COMPLETED:
        if timeout and (time.time() - start_time) > timeout:
            return False
        time.sleep(0.1)
    
    return True


# ==================== æ”¯æŒWarmupé™çº§çš„ä¾¿æ·å‡½æ•° ====================

def is_document_uploaded_with_warmup(doc_id: str, fallback_to_db: bool = False) -> tuple:
    """
    æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ä¸Šä¼ ï¼ˆæ”¯æŒWarmupé™çº§ï¼‰
    
    ç”±äºå¸ƒéš†è¿‡æ»¤å™¨å­˜åœ¨è¯¯åˆ¤ï¼ˆfalse positiveï¼‰ï¼Œå½“å¸ƒéš†è¿‡æ»¤å™¨è¡¨ç¤ºå­˜åœ¨æ—¶ï¼Œ
    ä¼šè¿›è¡ŒåŒé‡éªŒè¯ï¼šä»æ•°æ®åº“äºŒæ¬¡ç¡®è®¤ï¼Œé¿å…è¯¯åˆ¤å¯¼è‡´ç”¨æˆ·æ— æ³•ä¸Šä¼ æ–°æ–‡æ¡£
    
    Args:
        doc_id: æ–‡æ¡£IDï¼ˆMD5 hashï¼‰
        fallback_to_db: å¦‚æœWarmupæœªå®Œæˆï¼Œæ˜¯å¦ä»æ•°æ®åº“æŸ¥è¯¢ï¼ˆé™çº§ç­–ç•¥ï¼‰
    
    Returns:
        tuple: (exists: bool, used_db: bool)
            - exists: æ˜¯å¦å­˜åœ¨
            - used_db: æ˜¯å¦ä½¿ç”¨äº†æ•°æ®åº“æŸ¥è¯¢ï¼ˆé™çº§æˆ–åŒé‡éªŒè¯ï¼‰
    """
    # å¦‚æœWarmupå·²å®Œæˆï¼Œä½¿ç”¨å¸ƒéš†è¿‡æ»¤å™¨ + åŒé‡éªŒè¯
    if BLOOM_WARMUP_COMPLETED:
        exists_in_bloom = document_bloom.exists(doc_id)
        
        # å¸ƒéš†è¿‡æ»¤å™¨è¯´ä¸å­˜åœ¨ï¼Œè‚¯å®šä¸å­˜åœ¨ï¼ˆæ— å‡é˜´æ€§ï¼‰
        if not exists_in_bloom:
            return False, False
        
        # å¸ƒéš†è¿‡æ»¤å™¨è¯´å¯èƒ½å­˜åœ¨ï¼Œéœ€è¦åŒé‡éªŒè¯ï¼ˆé¿å…å‡é˜³æ€§è¯¯åˆ¤ï¼‰
        # ä»æ•°æ®åº“ç¡®è®¤æ˜¯å¦çœŸçš„å­˜åœ¨
        try:
            from pymongo import MongoClient
            from config import MONGO_URI, MONGO_DB_NAME
            
            client = MongoClient(MONGO_URI)
            db = client[MONGO_DB_NAME]
            
            # æŸ¥è¯¢æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            doc = db.documents.find_one({"file_hash": doc_id})
            exists = doc is not None
            
            client.close()
            return exists, True  # ä½¿ç”¨äº†æ•°æ®åº“åŒé‡éªŒè¯
        except Exception as e:
            logger.error(f"âŒ åŒé‡éªŒè¯æ–‡æ¡£å¤±è´¥: {e}")
            # åŒé‡éªŒè¯å¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼šè¿”å›Falseï¼ˆå…è®¸ä¸Šä¼ ï¼Œåç»­ä¼šå†æ¬¡éªŒè¯ï¼‰
            return False, True
    
    # Warmupæœªå®Œæˆï¼Œå¤„ç†é™çº§ç­–ç•¥
    if fallback_to_db:
        # ä»MongoDBæŸ¥è¯¢ï¼ˆé™çº§åˆ°æ•°æ®åº“ï¼‰
        try:
            from pymongo import MongoClient
            from config import MONGO_URI, MONGO_DB_NAME
            
            client = MongoClient(MONGO_URI)
            db = client[MONGO_DB_NAME]
            
            # æŸ¥è¯¢æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            doc = db.documents.find_one({"file_hash": doc_id})
            exists = doc is not None
            
            client.close()
            return exists, True
        except Exception as e:
            logger.error(f"âŒ é™çº§æŸ¥è¯¢æ–‡æ¡£å¤±è´¥: {e}")
            # é™çº§å¤±è´¥ï¼Œè¿”å›Falseï¼ˆå‡è®¾ä¸å­˜åœ¨ï¼‰
            return False, True
    
    # ä¸ä½¿ç”¨é™çº§ï¼Œç›´æ¥è¿”å›Falseï¼ˆå‡è®¾ä¸å­˜åœ¨ï¼‰
    return False, False


def is_cache_key_exists_with_warmup(key: str, fallback_to_db: bool = False) -> tuple:
    """
    æ£€æŸ¥ç¼“å­˜é”®æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æŒWarmupé™çº§ï¼‰
    
    æ³¨æ„ï¼šè¿™é‡Œçš„"ç¼“å­˜é”®"æŒ‡çš„æ˜¯qr_login_statusç­‰éœ€è¦åœ¨Neo4jæ•°æ®åº“ä¸­æŸ¥è¯¢çš„å®é™…æ•°æ®ï¼Œ
    è€Œä¸æ˜¯Redisç¼“å­˜ã€‚é™çº§æ—¶ä¼šä»Neo4jæ•°æ®åº“æŸ¥è¯¢çœŸå®æ•°æ®
    
    Args:
        key: ç¼“å­˜é”®åï¼ˆæ ¼å¼å¦‚ "qr_login_status:username"ï¼‰
        fallback_to_db: å¦‚æœWarmupæœªå®Œæˆï¼Œæ˜¯å¦ä»æ•°æ®åº“æŸ¥è¯¢ï¼ˆé™çº§ç­–ç•¥ï¼‰
    
    Returns:
        tuple: (exists: bool, used_db: bool)
            - exists: æ˜¯å¦å­˜åœ¨ï¼ˆå¯¹äºqr_login_status:usernameï¼Œè¡¨ç¤ºè¯¥ç”¨æˆ·å¯ç”¨äº†äºŒç»´ç ç™»å½•ï¼‰
            - used_db: æ˜¯å¦ä½¿ç”¨äº†æ•°æ®åº“æŸ¥è¯¢ï¼ˆé™çº§ï¼‰
    """
    # å¦‚æœWarmupå·²å®Œæˆï¼Œä½¿ç”¨å¸ƒéš†è¿‡æ»¤å™¨
    if BLOOM_WARMUP_COMPLETED:
        return cache_keys_bloom.exists(key), False
    
    # Warmupæœªå®Œæˆï¼Œå¤„ç†é™çº§ç­–ç•¥
    if fallback_to_db:
        # ä»Neo4jæ•°æ®åº“æŸ¥è¯¢çœŸå®æ•°æ®ï¼ˆè€Œä¸æ˜¯Redisç¼“å­˜ï¼‰
        try:
            from database import driver
            
            if not driver:
                logger.warning("âš ï¸ Neo4jè¿æ¥æœªå»ºç«‹ï¼Œé™çº§å¤±è´¥")
                return False, True
            
            # è§£æç¼“å­˜é”®ï¼Œæå–username
            # ç¼“å­˜é”®æ ¼å¼: "qr_login_status:username"
            if key.startswith("qr_login_status:"):
                username = key.split(":")[-1]
                
                with driver.session() as session:
                    query = """
                    MATCH (u:User {username: $username})
                    WHERE u.qr_login_enabled IS NOT NULL
                    RETURN u.qr_login_enabled as qr_login_enabled
                    """
                    result = session.run(query, username=username).single()
                    
                    if result:
                        qr_login_enabled = result.get("qr_login_enabled", False)
                        if qr_login_enabled:
                            # æ ‡è®°åˆ°å¸ƒéš†è¿‡æ»¤å™¨ï¼ˆåç»­æŸ¥è¯¢å¯ä»¥ç›´æ¥ç”¨å¸ƒéš†è¿‡æ»¤å™¨ï¼‰
                            mark_cache_key_exists(key)
                            return True, True
            
            # å…¶ä»–ç±»å‹çš„ç¼“å­˜é”®æš‚ä¸æ”¯æŒé™çº§ï¼Œè¿”å›False
            return False, True
            
        except Exception as e:
            logger.error(f"âŒ é™çº§æŸ¥è¯¢ç¼“å­˜é”®å¤±è´¥: {e}")
            # é™çº§å¤±è´¥ï¼Œè¿”å›Falseï¼ˆå‡è®¾ä¸å­˜åœ¨ï¼‰
            return False, True
    
    # ä¸ä½¿ç”¨é™çº§ï¼Œç›´æ¥è¿”å›Falseï¼ˆå‡è®¾ä¸å­˜åœ¨ï¼‰
    return False, False
