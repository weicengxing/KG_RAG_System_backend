"""
Flink éŸ³ä¹æ’­æ”¾æµå¤„ç†ä½œä¸šï¼ˆç”Ÿäº§çº§ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
ä» Kafka æ¶ˆè´¹æ’­æ”¾äº‹ä»¶ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—çƒ­åº¦ï¼Œè¾“å‡ºåˆ° Redis

ä¼˜åŒ–æ”¹è¿›ï¼š
1. âœ… æ·»åŠ  Watermark ç­–ç•¥ï¼Œè§£å†³çª—å£æ— æ³•è§¦å‘é—®é¢˜
2. âœ… ä½¿ç”¨ AggregateFunction å¢é‡è®¡ç®—ï¼Œé¿å… OOM
3. âœ… ä¼˜åŒ– Redis å†™å…¥ï¼Œæ·»åŠ  Top N é™åˆ¶
4. âœ… æ”¹è¿›æ•°æ®ç±»å‹ï¼Œä½¿ç”¨ Row æ›¿ä»£ PICKLED_BYTE_ARRAY
"""

import json
import logging
import time
from typing import Dict, List

import redis
from pyflink.common import Types, Row
from pyflink.common.time import Duration
from pyflink.common.watermark_strategy import WatermarkStrategy
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer, KafkaOffsetsInitializer
from pyflink.datastream.functions import (
    AggregateFunction,
    ProcessWindowFunction,
    RuntimeContext,
    MapFunction
)
from pyflink.datastream.window import SlidingEventTimeWindows

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== é…ç½®å‚æ•° ====================

KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'
MUSIC_PLAY_EVENTS_TOPIC = 'music-play-events'
CONSUMER_GROUP_ID = 'flink-music-trending-consumer'

# çª—å£é…ç½®
WINDOW_SIZE_SECONDS = 3600  # 1å°æ—¶çª—å£
WINDOW_SLIDE_SECONDS = 300  # 5åˆ†é’Ÿæ»‘åŠ¨

# Redis é…ç½®
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_DB = 0
REDIS_PASSWORD = None

# Top N é…ç½®
TOP_N_COUNT = 1000  # åªä¿ç•™å‰1000é¦–çƒ­é—¨æ­Œæ›²

# Watermark é…ç½®
WATERMARK_DELAY_SECONDS = 10  # å…è®¸10ç§’çš„ä¹±åº

# ==================== Redis è¿æ¥ ====================

try:
    redis_client = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        db=REDIS_DB,
        password=REDIS_PASSWORD,
        decode_responses=False  # ä¿æŒ bytes æ ¼å¼
    )
    redis_client.ping()
    logger.info(f"âœ… Flink Redis è¿æ¥æˆåŠŸ: {REDIS_HOST}:{REDIS_PORT}")
except Exception as e:
    logger.error(f"âŒ Flink Redis è¿æ¥å¤±è´¥: {e}")
    redis_client = None


# ==================== æ•°æ®ç»“æ„å®šä¹‰ ====================

class PlayEvent:
    """æ’­æ”¾äº‹ä»¶æ•°æ®ç»“æ„"""
    
    def __init__(self, event_id: str, song_id: int, user_id: str, timestamp: int, event_type: str):
        self.event_id = event_id
        self.song_id = song_id
        self.user_id = user_id
        self.timestamp = timestamp  # æ¯«ç§’æ—¶é—´æˆ³
        self.event_type = event_type
        self.timestamp_sec = timestamp / 1000.0  # è½¬æ¢ä¸ºç§’
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'event_id': self.event_id,
            'song_id': self.song_id,
            'user_id': self.user_id,
            'timestamp': self.timestamp,
            'event_type': self.event_type
        }
    
    @staticmethod
    def from_json(json_str: str):
        """ä» JSON å­—ç¬¦ä¸²è§£æ"""
        try:
            data = json.loads(json_str)
            return PlayEvent(
                event_id=data.get('event_id', ''),
                song_id=data.get('song_id', 0),
                user_id=data.get('user_id', ''),
                timestamp=data.get('timestamp', int(time.time() * 1000)),
                event_type=data.get('event_type', 'play')
            )
        except Exception as e:
            logger.error(f"è§£ææ’­æ”¾äº‹ä»¶å¤±è´¥: {e}")
            return None


class SongHotness:
    """æ­Œæ›²çƒ­åº¦ç»“æœ"""
    
    def __init__(self, song_id: int, hotness: float, play_count: int, window_end: int):
        self.song_id = song_id
        self.hotness = hotness
        self.play_count = play_count
        self.window_end = window_end


# ==================== MapFunctionï¼šè§£æ JSON å¹¶è½¬æ¢ä¸º Row ====================

class EventParserMapFunction(MapFunction):
    """å°† JSON å­—ç¬¦ä¸²è§£æä¸º PlayEvent å¹¶è½¬æ¢ä¸º Row"""
    
    def map(self, value: str) -> Row:
        """Map å‡½æ•°"""
        try:
            event = PlayEvent.from_json(value)
            if event:
                # è½¬æ¢ä¸º Row æ ¼å¼ï¼Œé¿å…ä½¿ç”¨ PICKLED_BYTE_ARRAY
                return Row(
                    event.event_id,
                    event.song_id,
                    event.user_id,
                    event.timestamp,
                    event.event_type,
                    event.timestamp_sec
                )
            else:
                return None
        except Exception as e:
            logger.error(f"è§£æäº‹ä»¶å¤±è´¥: {e}")
            return None


# ==================== AggregateFunctionï¼šå¢é‡è®¡ç®—çƒ­åº¦ ====================

class HotnessAggregateFunction(AggregateFunction):
    """çƒ­åº¦èšåˆå‡½æ•°ï¼ˆå¢é‡è®¡ç®—ï¼Œé¿å… OOMï¼‰"""
    
    def create_accumulator(self):
        """åˆ›å»ºç´¯åŠ å™¨ï¼š[æ’­æ”¾æ¬¡æ•°, æ—¶é—´æˆ³æ€»å’Œ]"""
        return (0, 0)
    
    def add(self, element: Row, accumulator):
        """æ·»åŠ å…ƒç´ åˆ°ç´¯åŠ å™¨"""
        play_count, timestamp_sum = accumulator
        return (play_count + 1, timestamp_sum + element.timestamp_sec)
    
    def merge(self, acc_a, acc_b):
        """åˆå¹¶ç´¯åŠ å™¨"""
        count_a, sum_a = acc_a
        count_b, sum_b = acc_b
        return (count_a + count_b, sum_a + sum_b)
    
    def get_result(self, accumulator):
        """è·å–èšåˆç»“æœ"""
        play_count, timestamp_sum = accumulator
        return (play_count, timestamp_sum)


# ==================== ProcessWindowFunctionï¼šè®¡ç®—çƒ­åº¦å¹¶å†™å…¥ Redis ====================

class HotnessProcessorFunction(ProcessWindowFunction):
    """çƒ­åº¦å¤„ç†å‡½æ•°ï¼ˆçª—å£å‡½æ•°ï¼‰"""
    
    def __init__(self):
        super().__init__()
        self.redis_client = None
    
    def open(self, runtime_context: RuntimeContext):
        """åˆå§‹åŒ– Redis è¿æ¥"""
        try:
            self.redis_client = redis.Redis(
                host=REDIS_HOST,
                port=REDIS_PORT,
                db=REDIS_DB,
                password=REDIS_PASSWORD,
                decode_responses=False
            )
            logger.info("âœ… Flink çª—å£å‡½æ•° Redis è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Flink çª—å£å‡½æ•° Redis è¿æ¥å¤±è´¥: {e}")
    
    def process(self, key, context, elements: List):
        """å¤„ç†çª—å£æ•°æ®
        
        Args:
            key: song_id (å­—ç¬¦ä¸²ç±»å‹)
            context: çª—å£ä¸Šä¸‹æ–‡
            elements: èšåˆç»“æœåˆ—è¡¨ [(play_count, timestamp_sum), ...]
        """
        song_id = int(key)
        
        if not elements:
            return
        
        # è·å–èšåˆç»“æœ
        play_count, timestamp_sum = elements[0]
        
        # è·å–çª—å£ç»“æŸæ—¶é—´
        window_end = context.window().end
        
        # è®¡ç®—çƒ­åº¦
        hotness = self._calculate_hotness(play_count, timestamp_sum, window_end)
        
        logger.info(f"ğŸ“Š çª—å£è®¡ç®—: song_id={song_id}, play_count={play_count}, hotness={hotness:.2f}")
        
        # è¾“å‡ºåˆ° Redis
        self._write_to_redis(song_id, hotness)
    
    def _calculate_hotness(self, play_count: int, timestamp_sum: float, window_end: int) -> float:
        """è®¡ç®—çƒ­åº¦åˆ†æ•°
        
        çƒ­åº¦ç®—æ³•ï¼šçƒ­åº¦ = æ’­æ”¾æ¬¡æ•° Ã— æ—¶é—´è¡°å‡å› å­ Ã— çª—å£æ—¶é—´è¡°å‡
        
        Args:
            play_count: æ’­æ”¾æ¬¡æ•°
            timestamp_sum: æ—¶é—´æˆ³æ€»å’Œï¼ˆç§’ï¼‰
            window_end: çª—å£ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            
        Returns:
            float: çƒ­åº¦åˆ†æ•°
        """
        if play_count == 0:
            return 0.0
        
        # è®¡ç®—å¹³å‡æ’­æ”¾æ—¶é—´ï¼ˆç§’ï¼‰
        avg_play_time = timestamp_sum / play_count
        
        # çª—å£ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
        window_end_sec = window_end / 1000.0
        
        # è®¡ç®—æ—¶é—´è¡°å‡å› å­ï¼ˆè¶Šæ¥è¿‘çª—å£æœ«å°¾ï¼Œè¡°å‡å› å­è¶Šå¤§ï¼‰
        # è¡°å‡å› å­èŒƒå›´ï¼š[0, 1]ï¼Œæœ€è¿‘æ’­æ”¾çš„æ­Œæ›²æ¥è¿‘1ï¼Œå¾ˆä¹…å‰çš„æ¥è¿‘0
        decay_age_seconds = 300  # 5åˆ†é’Ÿè¡°å‡å½±å“èŒƒå›´
        time_diff = window_end_sec - avg_play_time
        decay_factor = max(0, 1 - (time_diff / decay_age_seconds))
        
        # çƒ­åº¦ = æ’­æ”¾æ¬¡æ•° Ã— æ—¶é—´è¡°å‡å› å­
        hotness = play_count * decay_factor
        
        # æ·»åŠ é¢å¤–çš„æ—¶é—´è¡°å‡ï¼ˆçª—å£å†…å¹³å‡æ—¶é—´è¶Šæ¥è¿‘çª—å£æœ«å°¾ï¼Œçƒ­åº¦è¶Šé«˜ï¼‰
        window_age = window_end_sec - avg_play_time
        time_decay = max(0.1, 1 - (window_age / WINDOW_SIZE_SECONDS))
        
        hotness = hotness * time_decay
        
        return hotness
    
    def _write_to_redis(self, song_id: int, hotness: float):
        """å†™å…¥ Redisï¼ˆä¼˜åŒ–ç‰ˆï¼šæ·»åŠ  Top N é™åˆ¶ï¼‰
        
        Args:
            song_id: æ­Œæ›²ID
            hotness: çƒ­åº¦åˆ†æ•°
        """
        if not self.redis_client:
            logger.warning("âš ï¸ Redis æœªè¿æ¥ï¼Œæ— æ³•å†™å…¥çƒ­åº¦")
            return
        
        try:
            key = "music:trending:hot"
            
            # æ·»åŠ åˆ° ZSET
            self.redis_client.zadd(key, {str(song_id): hotness})
            
            # åªä¿ç•™å‰ TOP_N_COUNT é¦–æ­Œæ›²ï¼ˆé¿å…æ— é™å¢é•¿ï¼‰
            self.redis_client.zremrangebyrank(key, 0, -TOP_N_COUNT - 1)
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è®¾ç½®æ—¶ï¼‰
            if self.redis_client.ttl(key) == -1:  # æ²¡æœ‰è®¾ç½®è¿‡æœŸæ—¶é—´
                self.redis_client.expire(key, 6 * 3600)  # 6å°æ—¶
            
            logger.debug(f"âœ… å†™å…¥ Redis: song_id={song_id}, hotness={hotness:.2f}")
        except Exception as e:
            logger.error(f"âŒ å†™å…¥ Redis å¤±è´¥: song_id={song_id}, {e}")
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.redis_client:
            try:
                self.redis_client.close()
                logger.info("ğŸ”’ Flink çª—å£å‡½æ•° Redis è¿æ¥å·²å…³é—­")
            except Exception as e:
                logger.error(f"âŒ å…³é—­ Redis è¿æ¥å¤±è´¥: {e}")


# ==================== Flink ä½œä¸šä¸»å‡½æ•° ====================

def create_music_hotness_job():
    """åˆ›å»ºéŸ³ä¹çƒ­åº¦è®¡ç®—ä½œä¸š"""
    
    # åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # é…ç½® Checkpointï¼ˆæ•…éšœæ¢å¤ï¼‰
    env.enable_checkpointing(60000)  # æ¯ 60 ç§’åšä¸€æ¬¡ checkpoint
    env.get_checkpoint_config().set_checkpoint_timeout(300000)  # checkpoint è¶…æ—¶æ—¶é—´ 5 åˆ†é’Ÿ
    env.get_checkpoint_config().set_min_pause_between_checkpoints(30000)  # checkpoint æœ€å°é—´éš” 30 ç§’
    
    logger.info("âœ… Flink æ‰§è¡Œç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    
    # é…ç½® Kafka Consumer
    kafka_props = {
        'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
        'group.id': CONSUMER_GROUP_ID,
        'auto.offset.reset': 'latest',  # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹
    }
    
    kafka_consumer = FlinkKafkaConsumer(
        topics=MUSIC_PLAY_EVENTS_TOPIC,
        deserialization_schema=Types.STRING(),
        properties=kafka_props,
        starting_offsets=KafkaOffsetsInitializer.latest()
    )
    
    logger.info(f"âœ… Kafka Consumer åˆ›å»ºæˆåŠŸ: {MUSIC_PLAY_EVENTS_TOPIC}")
    
    # åˆ›å»ºæ•°æ®æµ
    stream = env.add_source(kafka_consumer)
    
    # è§£æ JSON å­—ç¬¦ä¸²ä¸º Row æ ¼å¼
    parsed_stream = stream.map(EventParserMapFunction(), output_type=Types.ROW(
        [Types.STRING(), Types.INT(), Types.STRING(), Types.LONG(), Types.STRING(), Types.FLOAT()]
    ))
    
    # è¿‡æ»¤æ— æ•ˆäº‹ä»¶
    valid_stream = parsed_stream.filter(lambda x: x is not None)
    
    # âœ… å…³é”®ä¿®å¤ï¼šæ·»åŠ  Watermark ç­–ç•¥
    watermark_strategy = (
        WatermarkStrategy
        .for_bounded_out_of_orderness(Duration.of_seconds(WATERMARK_DELAY_SECONDS))
        .with_timestamp_assigner(lambda event, timestamp: event[3])  # ä½¿ç”¨ç¬¬3ä¸ªå­—æ®µï¼štimestamp (æ¯«ç§’)
        .with_idleness(Duration.of_minutes(5))  # 5åˆ†é’Ÿæ— æ•°æ®è§†ä¸ºç©ºé—²åˆ†åŒº
    )
    
    valid_stream = valid_stream.assign_timestamps_and_watermarks(watermark_strategy)
    logger.info("âœ… Watermark ç­–ç•¥å·²æ·»åŠ ")
    
    # æŒ‰ song_id åˆ†åŒº
    keyed_stream = valid_stream.key_by(lambda e: str(e[1]), key_type=Types.STRING())  # ä½¿ç”¨ç¬¬1ä¸ªå­—æ®µï¼šsong_id
    
    # åº”ç”¨æ»‘åŠ¨çª—å£ï¼ˆ1å°æ—¶çª—å£ï¼Œ5åˆ†é’Ÿæ»‘åŠ¨ï¼‰
    windowed_stream = keyed_stream.window(
        SlidingEventTimeWindows.of(
            size_ms=Duration.of_seconds(WINDOW_SIZE_SECONDS).to_milliseconds(),
            slide_ms=Duration.of_seconds(WINDOW_SLIDE_SECONDS).to_milliseconds()
        )
    )
    
    # âœ… ä½¿ç”¨ AggregateFunction + ProcessWindowFunction ç»„åˆï¼ˆé¿å… OOMï¼‰
    # AggregateFunction å¢é‡è®¡ç®—ï¼ŒProcessWindowFunction åªå¤„ç†èšåˆç»“æœ
    hotness_stream = windowed_stream.aggregate(
        HotnessAggregateFunction(),
        HotnessProcessorFunction()
    )
    
    logger.info("âœ… Flink ä½œä¸šæ„å»ºå®Œæˆï¼ˆç”Ÿäº§çº§ä¼˜åŒ–ç‰ˆï¼‰")
    
    return env


# ==================== è¾…åŠ©å‡½æ•° ====================

def submit_job_to_cluster(jar_path: str = None):
    """æäº¤ä½œä¸šåˆ° Flink é›†ç¾¤
    
    Args:
        jar_path: Flink Python ä½œä¸š jar åŒ…è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    try:
        # åˆ›å»ºä½œä¸š
        env = create_music_hotness_job()
        
        # æ‰§è¡Œä½œä¸š
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œ Flink ä½œä¸š...")
        env.execute("Music Hotness Calculator (Production)")
        
    except Exception as e:
        logger.error(f"âŒ Flink ä½œä¸šæ‰§è¡Œå¤±è´¥: {e}")
        raise


def run_local():
    """æœ¬åœ°è¿è¡Œæ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    try:
        # åˆ›å»ºä½œä¸š
        env = create_music_hotness_job()
        
        # æ‰§è¡Œä½œä¸š
        logger.info("ğŸš€ æœ¬åœ°è¿è¡Œ Flink ä½œä¸š...")
        env.execute("Music Hotness Calculator (Local)")
        
    except Exception as e:
        logger.error(f"âŒ æœ¬åœ°è¿è¡Œå¤±è´¥: {e}")
        raise


# ==================== æµ‹è¯•å‡½æ•° ====================

def test_flink_job():
    """æµ‹è¯• Flink ä½œä¸š
    
    Returns:
        bool: æµ‹è¯•æ˜¯å¦æˆåŠŸ
    """
    try:
        logger.info("å¼€å§‹æµ‹è¯• Flink ä½œä¸š...")
        
        # æµ‹è¯• PlayEvent è§£æ
        test_json = json.dumps({
            'event_id': 'test-001',
            'song_id': 1,
            'user_id': 'test_user',
            'timestamp': int(time.time() * 1000),
            'event_type': 'play'
        })
        
        event = PlayEvent.from_json(test_json)
        if event:
            logger.info(f"âœ… äº‹ä»¶è§£ææˆåŠŸ: {event.to_dict()}")
        else:
            logger.error("âŒ äº‹ä»¶è§£æå¤±è´¥")
            return False
        
        # æµ‹è¯• AggregateFunction
        agg_func = HotnessAggregateFunction()
        accumulator = agg_func.create_accumulator()
        logger.info(f"âœ… AggregateFunction åˆ›å»ºæˆåŠŸ: {accumulator}")
        
        # æµ‹è¯• Redis è¿æ¥
        if redis_client:
            redis_client.ping()
            logger.info("âœ… Redis è¿æ¥æ­£å¸¸")
        else:
            logger.warning("âš ï¸ Redis æœªè¿æ¥")
        
        logger.info("âœ… Flink ä½œä¸šæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Flink ä½œä¸šæµ‹è¯•å¤±è´¥: {e}")
        return False


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    import sys
    
    # æ—¥å¿—é…ç½®
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        mode = sys.argv[1]
    else:
        mode = 'local'  # é»˜è®¤æœ¬åœ°è¿è¡Œ
    
    logger.info(f"ğŸµ Flink éŸ³ä¹çƒ­åº¦è®¡ç®—ä½œä¸šå¯åŠ¨æ¨¡å¼: {mode}")
    logger.info(f"ğŸ“Š ç”Ÿäº§çº§ä¼˜åŒ–ç‰ˆæœ¬ï¼šWatermark + AggregateFunction + Top N é™åˆ¶")
    
    # æµ‹è¯•æ¨¡å¼
    if mode == 'test':
        if test_flink_job():
            logger.info("âœ… æµ‹è¯•æˆåŠŸ")
            sys.exit(0)
        else:
            logger.error("âŒ æµ‹è¯•å¤±è´¥")
            sys.exit(1)
    
    # æäº¤åˆ°é›†ç¾¤
    elif mode == 'cluster':
        try:
            submit_job_to_cluster()
            logger.info("âœ… ä½œä¸šå·²æäº¤åˆ°é›†ç¾¤")
        except Exception as e:
            logger.error(f"âŒ æäº¤ä½œä¸šå¤±è´¥: {e}")
            sys.exit(1)
    
    # æœ¬åœ°è¿è¡Œï¼ˆé»˜è®¤ï¼‰
    else:
        try:
            run_local()
            logger.info("âœ… ä½œä¸šæ‰§è¡Œå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°è¿è¡Œå¤±è´¥: {e}")
            sys.exit(1)
