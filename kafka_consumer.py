"""
Kafka æ¶ˆè´¹è€…æœåŠ¡
ç”¨äºå¼‚æ­¥å¤„ç†çŸ¥è¯†å›¾è°±æ–‡æ¡£æ„å»ºä»»åŠ¡
"""

import sys
import json
import time
import asyncio
import logging
from confluent_kafka import Consumer, KafkaException, KafkaError, AdminClient, NewTopic
from task_manager import task_manager
from kg_service import kg_service

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Kafka é…ç½®
KAFKA_CONFIG = {
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'kg-doc-processor',
    'auto.offset.reset': 'earliest',
    'enable.auto.commit': True
}


class DocProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
    
    async def process_document(self, task_id: str, doc_id: str, file_path: str, filename: str):
        """
        å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆè§£æã€åˆ†å—ã€æŠ½å–ã€å»ºå›¾è°±ã€å‘é‡åŒ–ï¼‰
        
        Args:
            task_id: ä»»åŠ¡ID
            doc_id: æ–‡æ¡£ID
            file_path: æ–‡ä»¶è·¯å¾„
            filename: æ–‡ä»¶å
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {filename} (doc_id={doc_id}, task_id={task_id})")
            
            # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹è§£æ
            task_manager.update_progress(task_id, 10, "parsing", f"æ­£åœ¨è§£ææ–‡æ¡£: {filename}")
            logger.info(f"ğŸ“– [10%] å¼€å§‹è§£ææ–‡æ¡£: {filename}")
            
            # 1. è§£ææ–‡æ¡£
            text = kg_service.parse_document(file_path)
            text_length = len(text)
            logger.info(f"âœ… [20%] æ–‡æ¡£è§£æå®Œæˆï¼Œæ–‡æœ¬é•¿åº¦: {text_length} å­—ç¬¦")
            
            # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹åˆ†å—
            task_manager.update_progress(task_id, 30, "chunking", "æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ†å—...")
            logger.info(f"ğŸ”ª [30%] å¼€å§‹æ–‡æœ¬åˆ†å—")
            
            # 2. æ–‡æœ¬åˆ†å—
            chunks = kg_service.split_text(text)
            logger.info(f"âœ… [40%] æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªåˆ†å—")
            
            # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹æŠ½å–å®ä½“å…³ç³»
            task_manager.update_progress(task_id, 50, "extracting", "æ­£åœ¨æŠ½å–å®ä½“å…³ç³»...")
            logger.info(f"ğŸ” [50%] å¼€å§‹æŠ½å–å®ä½“å…³ç³»")
            
            # 3. å¹¶å‘æŠ½å–å®ä½“å…³ç³»
            triplets = await kg_service.extract_batch_async(chunks)
            logger.info(f"âœ… [70%] å®ä½“å…³ç³»æŠ½å–å®Œæˆï¼Œå…± {len(triplets)} ä¸ªä¸‰å…ƒç»„")
            
            # æ›´æ–°è¿›åº¦ï¼šæŠ½å–å®Œæˆ
            task_manager.update_progress(task_id, 70, "extracting", f"å®ä½“å…³ç³»æŠ½å–å®Œæˆï¼Œå…± {len(triplets)} ä¸ªä¸‰å…ƒç»„")
            
            # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±
            task_manager.update_progress(task_id, 80, "building_graph", "æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
            logger.info(f"ğŸ•¸ï¸ [80%] å¼€å§‹ä¿å­˜åˆ° Neo4j")
            
            # 4. ä¿å­˜åˆ° Neo4j
            kg_service.save_triplets_to_neo4j(triplets, doc_id)
            logger.info(f"âœ… [85%] Neo4j å›¾è°±æ„å»ºå®Œæˆ")
            
            # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹å‘é‡åŒ–
            task_manager.update_progress(task_id, 90, "embedding", "æ­£åœ¨ç”Ÿæˆå‘é‡åµŒå…¥...")
            logger.info(f"ğŸ”¢ [90%] å¼€å§‹å‘é‡åŒ–å¹¶ä¿å­˜åˆ° ChromaDB")
            
            # 5. ä¿å­˜åˆ° ChromaDB
            kg_service.save_chunks_to_chromadb(chunks, doc_id)
            logger.info(f"âœ… [95%] ChromaDB å‘é‡åŒ–å®Œæˆ")
            
            # 6. è·å–å›¾è°±æ•°æ®ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰
            logger.info(f"ğŸ“Š [98%] è·å–å›¾è°±æ•°æ®")
            graph_data = kg_service.get_graph_data(doc_id, limit=100)
            
            # è®¡ç®—è€—æ—¶
            elapsed_time = round(time.time() - start_time, 2)
            logger.info(f"â±ï¸ å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {elapsed_time} ç§’")
            
            # 7. æ ‡è®°ä»»åŠ¡å®Œæˆ
            result_data = {
                "doc_id": doc_id,
                "filename": filename,
                "total_chunks": len(chunks),
                "total_triplets": len(triplets),
                "elapsed_time": elapsed_time,
                "graph_data": graph_data,
                "text_length": text_length
            }
            
            task_manager.complete_task(task_id, result_data)
            logger.info(f"âœ…âœ…âœ… ä»»åŠ¡å®Œæˆ: {task_id}")
            
            return result_data
            
        except Exception as e:
            error_msg = f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            
            # æ ‡è®°ä»»åŠ¡å¤±è´¥
            task_manager.fail_task(task_id, error_msg)
            
            raise e
    
    def run_async_task(self, task_id: str, doc_id: str, file_path: str, filename: str):
        """
        åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            doc_id: æ–‡æ¡£ID
            file_path: æ–‡ä»¶è·¯å¾„
            filename: æ–‡ä»¶å
        """
        try:
            # åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
            result = self.loop.run_until_complete(
                self.process_document(task_id, doc_id, file_path, filename)
            )
            return result
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def close(self):
        """å…³é—­äº‹ä»¶å¾ªç¯"""
        self.loop.close()


def create_topic_if_not_exists(topic_name: str, num_partitions: int = 3, replication_factor: int = 1):
    """
    è‡ªåŠ¨åˆ›å»º Kafka Topicï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    
    Args:
        topic_name: Topic åç§°
        num_partitions: åˆ†åŒºæ•°
        replication_factor: å‰¯æœ¬å› å­
    """
    try:
        # åˆ›å»º AdminClient
        admin_client = AdminClient({'bootstrap.servers': 'localhost:9092'})
        
        # æ£€æŸ¥ Topic æ˜¯å¦å­˜åœ¨
        metadata = admin_client.list_topics(timeout=10)
        if topic_name in metadata.topics:
            logger.info(f"âœ… Topic '{topic_name}' å·²å­˜åœ¨")
            return True
        
        # åˆ›å»ºæ–° Topic
        logger.info(f"ğŸ“ æ­£åœ¨åˆ›å»º Topic '{topic_name}'...")
        new_topic = NewTopic(
            topic_name,
            num_partitions=num_partitions,
            replication_factor=replication_factor
        )
        
        future = admin_client.create_topics([new_topic])
        
        # ç­‰å¾…åˆ›å»ºå®Œæˆ
        for name, f in future.items():
            try:
                f.result()
                logger.info(f"âœ… Topic '{name}' åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ åˆ›å»º Topic '{name}' å¤±è´¥: {str(e)}")
                return False
        
        return True
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥/åˆ›å»º Topic å¤±è´¥: {str(e)}")
        logger.info("â„¹ï¸ è¯·æ‰‹åŠ¨åˆ›å»º Topic: bin/kafka-topics.sh --create --topic doc-upload --bootstrap-server localhost:9092")
        return False


def consume_messages():
    """
    æ¶ˆè´¹ Kafka æ¶ˆæ¯çš„ä¸»å‡½æ•°
    """
    # è‡ªåŠ¨åˆ›å»º Topicï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    topic = 'doc-upload'
    create_topic_if_not_exists(topic, num_partitions=3, replication_factor=1)
    
    # åˆ›å»ºæ¶ˆè´¹è€…
    consumer = Consumer(KAFKA_CONFIG)
    
    # è®¢é˜…ä¸»é¢˜
    consumer.subscribe([topic])
    
    logger.info("ğŸ§ Kafka æ¶ˆè´¹è€…å·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬ä¸»é¢˜: " + topic)
    
    # åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
    processor = DocProcessor()
    
    try:
        while True:
            # è½®è¯¢æ¶ˆæ¯ï¼ˆè¶…æ—¶ 1 ç§’ï¼‰
            msg = consumer.poll(1.0)
            
            if msg is None:
                # æ²¡æœ‰æ¶ˆæ¯ï¼Œç»§ç»­ç­‰å¾…
                continue
            
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    # åˆ°è¾¾åˆ†åŒºæœ«å°¾ï¼Œä¸æ˜¯é”™è¯¯
                    continue
                else:
                    # å…¶ä»–é”™è¯¯
                    logger.error(f"âŒ Kafka æ¶ˆè´¹é”™è¯¯: {msg.error()}")
                    raise KafkaException(msg.error())
            
            # æˆåŠŸè·å–æ¶ˆæ¯
            try:
                # è§£ææ¶ˆæ¯
                kafka_message = json.loads(msg.value().decode('utf-8'))
                task_id = kafka_message.get('task_id')
                doc_id = kafka_message.get('doc_id')
                file_path = kafka_message.get('file_path')
                filename = kafka_message.get('filename')
                
                logger.info(f"ğŸ“¥ æ”¶åˆ°æ¶ˆæ¯: task_id={task_id}, doc_id={doc_id}, filename={filename}")
                
                # å¤„ç†æ–‡æ¡£
                processor.run_async_task(task_id, doc_id, file_path, filename)
                
                # æ‰‹åŠ¨æäº¤åç§»é‡
                consumer.commit(msg)
                logger.info(f"âœ… åç§»é‡å·²æäº¤: topic={msg.topic()}, partition={msg.partition()}, offset={msg.offset()}")
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}")
                logger.exception(e)
    
    except KeyboardInterrupt:
        logger.info("â¸ï¸ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æ¶ˆè´¹è€…...")
    except Exception as e:
        logger.error(f"âŒ æ¶ˆè´¹è€…å¼‚å¸¸: {e}")
        logger.exception(e)
    finally:
        # å…³é—­èµ„æº
        processor.close()
        consumer.close()
        logger.info("ğŸ›‘ Kafka æ¶ˆè´¹è€…å·²å…³é—­")


if __name__ == '__main__':
    # å¯åŠ¨æ¶ˆè´¹è€…
    try:
        consume_messages()
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)
