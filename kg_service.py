"""
çŸ¥è¯†å›¾è°±RAGç³»ç»Ÿæ ¸å¿ƒæœåŠ¡
åŒ…å«æ–‡æ¡£å¤„ç†ã€å›¾è°±æ„å»ºã€å‘é‡å­˜å‚¨ã€RAGé—®ç­”ç­‰åŠŸèƒ½
"""

import os
import json
import asyncio
from typing import List, Dict, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor
import time
import hashlib


# æ–‡æ¡£å¤„ç†
from PyPDF2 import PdfReader

# æ–‡æœ¬åˆ†å—å™¨ - å…¼å®¹ä¸åŒç‰ˆæœ¬çš„langchain
try:
    # æ–°ç‰ˆæœ¬ langchain
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    try:
        # æ—§ç‰ˆæœ¬ langchain
        from langchain.text_splitter import RecursiveCharacterTextSplitter
    except ImportError:
        # å¦‚æœéƒ½ä¸è¡Œï¼Œä½¿ç”¨ç®€å•çš„åˆ†å—å®ç°
        class RecursiveCharacterTextSplitter:
            def __init__(self, chunk_size, chunk_overlap, **kwargs):
                self.chunk_size = chunk_size
                self.chunk_overlap = chunk_overlap

            def split_text(self, text):
                """ç®€å•çš„æ–‡æœ¬åˆ†å—å®ç°"""
                chunks = []
                start = 0
                text_length = len(text)

                while start < text_length:
                    end = start + self.chunk_size
                    chunk = text[start:end]
                    chunks.append(chunk)
                    start = end - self.chunk_overlap

                return chunks

# å‘é‡æ•°æ®åº“
import chromadb
from chromadb.config import Settings

# Neo4jå›¾æ•°æ®åº“
from neo4j import GraphDatabase

# LLMå’ŒEmbeddings
from openai import OpenAI


# å¼•å…¥é…ç½®å˜é‡
from config import (
    NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD,
    EXTRACTION_API_KEY, EXTRACTION_BASE_URL, EXTRACTION_MODEL,  # ä¸‰å…ƒç»„æå–ç›¸å…³
    QA_MODELS, DEFAULT_QA_MODEL,  # AIé—®ç­”æ¨¡å‹åˆ—è¡¨
    EMBED_API_KEY, EMBED_BASE_URL, EMBED_MODEL,  # Embedding ç›¸å…³
    CHUNK_SIZE, CHUNK_OVERLAP,
    VECTOR_SEARCH_TOP_K, VECTOR_SIMILARITY_THRESHOLD, GRAPH_SEARCH_HOPS
)


class KnowledgeGraphService:
    """çŸ¥è¯†å›¾è°±RAGæœåŠ¡ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        # Neo4jè¿æ¥
        self.neo4j_driver = GraphDatabase.driver(
            NEO4J_URI,
            auth=(NEO4J_USERNAME, NEO4J_PASSWORD)
        )

        # ChromaDBå®¢æˆ·ç«¯ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
        chroma_data_dir = os.path.join(os.path.dirname(__file__), "chroma_data")
        os.makedirs(chroma_data_dir, exist_ok=True)

        self.chroma_client = chromadb.PersistentClient(
            path=chroma_data_dir,
            settings=Settings(anonymized_telemetry=False)
        )

        # åˆ›å»ºæˆ–è·å–é›†åˆ
        self.collection = self.chroma_client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )

        # 3. åˆå§‹åŒ–ä¸‰å…ƒç»„æå–å®¢æˆ·ç«¯ (ä¸“ç”¨äºå®ä½“å…³ç³»æŠ½å–)
        self.extraction_client = OpenAI(
            api_key=EXTRACTION_API_KEY,
            base_url=EXTRACTION_BASE_URL
        )

        # 4. åˆå§‹åŒ– AI é—®ç­”å®¢æˆ·ç«¯å­—å…¸ (æ ¹æ®æ¨¡å‹åç§°å¿«é€ŸæŸ¥æ‰¾)
        self.qa_clients = {}
        for model_config in QA_MODELS:
            model_name = model_config["name"]
            self.qa_clients[model_name] = {
                "client": OpenAI(
                    api_key=model_config["api_key"],
                    base_url=model_config["base_url"]
                ),
                "model": model_config["model"]
            }

        # 5. åˆå§‹åŒ– Embedding å®¢æˆ·ç«¯ (ä½¿ç”¨ ModelScope é…ç½®)
        self.embed_client = OpenAI(
            api_key=EMBED_API_KEY,
            base_url=EMBED_BASE_URL
        )

        # æ–‡æœ¬åˆ†å—å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""]
        )

        # çº¿ç¨‹æ± ç”¨äºå¹¶å‘å¤„ç†
        self.executor = ThreadPoolExecutor(max_workers=10)

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'neo4j_driver'):
            self.neo4j_driver.close()

    def get_qa_client(self, model_name: Optional[str] = None):
        """è·å–AIé—®ç­”å®¢æˆ·ç«¯å’Œæ¨¡å‹åç§°"""
        if model_name is None:
            model_name = DEFAULT_QA_MODEL

        if model_name not in self.qa_clients:
            raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ¨¡å‹: {list(self.qa_clients.keys())}")

        return self.qa_clients[model_name]["client"], self.qa_clients[model_name]["model"]

    # ==================== æ–‡æ¡£å¤„ç† ====================

    def parse_pdf(self, file_path: str) -> str:
        """è§£æPDFæ–‡æ¡£"""
        try:
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
            return text
        except Exception as e:
            raise Exception(f"PDFè§£æå¤±è´¥: {str(e)}")

    def split_text(self, text: str) -> List[Dict[str, Any]]:
        """åˆ†å—æ–‡æœ¬"""
        chunks = self.text_splitter.split_text(text)

        # è¿”å›å¸¦æœ‰å…ƒæ•°æ®çš„åˆ†å—åˆ—è¡¨
        chunk_list = []
        for idx, chunk in enumerate(chunks):
            chunk_list.append({
                "index": idx,
                "content": chunk,
                "length": len(chunk)
            })

        return chunk_list

    # ==================== å®ä½“å…³ç³»æŠ½å– ====================

    def extract_entities_and_relations(self, text: str) -> List[Dict[str, str]]:
        """ä½¿ç”¨LLMæå–å®ä½“å’Œå…³ç³»ï¼ˆä¸‰å…ƒç»„ï¼‰"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œä»¥JSONæ ¼å¼è¾“å‡ºã€‚

è¦æ±‚ï¼š
1. è¯†åˆ«æ–‡æœ¬ä¸­çš„å…³é”®å®ä½“ï¼ˆäººç‰©ã€ç»„ç»‡ã€åœ°ç‚¹ã€æ¦‚å¿µç­‰ï¼‰
2. è¯†åˆ«å®ä½“ä¹‹é—´çš„å…³ç³»
3. è¾“å‡ºæ ¼å¼ä¸ºJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼šheadï¼ˆå¤´å®ä½“ï¼‰ã€relationï¼ˆå…³ç³»ï¼‰ã€tailï¼ˆå°¾å®ä½“ï¼‰ã€entity_typeï¼ˆå®ä½“ç±»å‹ï¼‰

ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
[
  {{"head": "å¼ ä¸‰", "relation": "ä»»èŒäº", "tail": "é˜¿é‡Œå·´å·´", "head_type": "Person", "tail_type": "Organization"}},
  {{"head": "é˜¿é‡Œå·´å·´", "relation": "ä½äº", "tail": "æ­å·", "head_type": "Organization", "tail_type": "Location"}}
]

æ–‡æœ¬å†…å®¹ï¼š
{text}

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—è¯´æ˜ï¼š"""

        try:
            response = self.extraction_client.chat.completions.create(
                model=EXTRACTION_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æ„å»ºåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )

            result_text = response.choices[0].message.content.strip()

            # è§£æJSON
            # å°è¯•æå–JSONï¼ˆæœ‰æ—¶LLMä¼šè¿”å›å¸¦ä»£ç å—çš„å†…å®¹ï¼‰
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()

            triplets = json.loads(result_text)

            # éªŒè¯æ ¼å¼
            if isinstance(triplets, list):
                return triplets
            else:
                return []

        except Exception as e:
            print(f"å®ä½“å…³ç³»æŠ½å–å¤±è´¥: {str(e)}")
            return []

    # è¯·ç¡®ä¿è¿™ä¸ªå‡½æ•°åœ¨ class KnowledgeGraphService: çš„ç¼©è¿›èŒƒå›´å†…ï¼ˆé€šå¸¸å‰é¢æœ‰4ä¸ªç©ºæ ¼ï¼‰
    async def extract_batch_async(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¹¶å‘å¤„ç†å¤šä¸ªæ–‡æœ¬å—ï¼Œä½†é™åˆ¶æœ€é«˜å¹¶å‘é‡å¹¶å¢åŠ æ§é€Ÿ"""
        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(8) 

        async def sem_task(chunk):
            async with semaphore:
                loop = asyncio.get_event_loop()
                # å¢åŠ å»¶è¿Ÿé˜²æ­¢è¯·æ±‚æ’è½¦
                await asyncio.sleep(0.5) 
                try:
                    # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„æŠ½å–å‡½æ•°
                    result = await loop.run_in_executor(
                        self.executor,
                        self.extract_entities_and_relations,
                        chunk["content"]
                    )
                    return result
                except Exception as e:
                    print(f"æŠ½å–å— {chunk['index']} å¤±è´¥: {e}")
                    return []

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [sem_task(chunk) for chunk in chunks]
        
        # å¹¶å‘ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks)

        # æ•´åˆç»“æœ
        all_triplets = []
        for idx, triplets_list in enumerate(results):
            if triplets_list and isinstance(triplets_list, list):
                for triplet in triplets_list:
                    # è®°å½•æ¥æºå—ç´¢å¼•
                    triplet["source_chunk_index"] = chunks[idx]["index"]
                    all_triplets.append(triplet)

        return all_triplets
    # ==================== Neo4jå›¾è°±å­˜å‚¨ ====================

    def save_triplets_to_neo4j(self, triplets: List[Dict[str, Any]], doc_id: str):
        """å°†ä¸‰å…ƒç»„ä¿å­˜åˆ°Neo4j"""

        with self.neo4j_driver.session() as session:
            # åˆ›å»ºæ–‡æ¡£èŠ‚ç‚¹
            session.run(
                "MERGE (d:Document {id: $doc_id})",
                doc_id=doc_id
            )

            # æ‰¹é‡åˆ›å»ºå®ä½“å’Œå…³ç³»
            for triplet in triplets:
                try:
                    head = triplet.get("head", "")
                    tail = triplet.get("tail", "")
                    relation = triplet.get("relation", "RELATED_TO")
                    head_type = triplet.get("head_type", "Entity")
                    tail_type = triplet.get("tail_type", "Entity")

                    if not head or not tail:
                        continue

                    # åˆ›å»ºå¤´å®ä½“
                    session.run(
                        f"MERGE (h:{head_type} {{name: $head}}) "
                        "ON CREATE SET h.created_at = timestamp() "
                        "MERGE (d:Document {id: $doc_id}) "
                        "MERGE (h)-[:FROM_DOCUMENT]->(d)",
                        head=head,
                        doc_id=doc_id
                    )

                    # åˆ›å»ºå°¾å®ä½“
                    session.run(
                        f"MERGE (t:{tail_type} {{name: $tail}}) "
                        "ON CREATE SET t.created_at = timestamp() "
                        "MERGE (d:Document {id: $doc_id}) "
                        "MERGE (t)-[:FROM_DOCUMENT]->(d)",
                        tail=tail,
                        doc_id=doc_id
                    )

                    # åˆ›å»ºå…³ç³»ï¼ˆåŠ¨æ€å…³ç³»ç±»å‹ï¼‰
                    # Neo4jä¸æ”¯æŒå‚æ•°åŒ–å…³ç³»ç±»å‹ï¼Œéœ€è¦ç”¨å­—ç¬¦ä¸²æ‹¼æ¥ï¼ˆæ³¨æ„å®‰å…¨æ€§ï¼‰
                    safe_relation = "".join([c if c.isalnum() or c == "_" else "_" for c in relation])
                    session.run(
                        f"MATCH (h {{name: $head}}) "
                        f"MATCH (t {{name: $tail}}) "
                        f"MERGE (h)-[r:{safe_relation}]->(t) "
                        "ON CREATE SET r.created_at = timestamp()",
                        head=head,
                        tail=tail
                    )

                except Exception as e:
                    print(f"ä¿å­˜ä¸‰å…ƒç»„å¤±è´¥: {triplet}, é”™è¯¯: {str(e)}")
                    continue

    def get_graph_data(self, doc_id: Optional[str] = None, limit: int = 100) -> Dict[str, Any]:
        """è·å–å›¾è°±æ•°æ®ï¼ˆç”¨äºå‰ç«¯å¯è§†åŒ–ï¼‰"""

        with self.neo4j_driver.session() as session:
            if doc_id:
                # æŸ¥è¯¢ç‰¹å®šæ–‡æ¡£çš„å›¾è°±
                result = session.run(
                    """
                    MATCH (n)-[r]->(m)
                    WHERE (n)-[:FROM_DOCUMENT]->(:Document {id: $doc_id})
                       OR (m)-[:FROM_DOCUMENT]->(:Document {id: $doc_id})
                    RETURN n, r, m
                    LIMIT $limit
                    """,
                    doc_id=doc_id,
                    limit=limit
                )
            else:
                # æŸ¥è¯¢æ‰€æœ‰å›¾è°±
                result = session.run(
                    """
                    MATCH (n)-[r]->(m)
                    WHERE NOT type(r) = 'FROM_DOCUMENT'
                    RETURN n, r, m
                    LIMIT $limit
                    """,
                    limit=limit
                )

            nodes = {}
            edges = []

            for record in result:
                n = record["n"]
                r = record["r"]
                m = record["m"]

                # èŠ‚ç‚¹
                if n.element_id not in nodes:
                    nodes[n.element_id] = {
                        "id": n.element_id,
                        "label": n.get("name", "Unknown"),
                        "type": list(n.labels)[0] if n.labels else "Entity"
                    }

                if m.element_id not in nodes:
                    nodes[m.element_id] = {
                        "id": m.element_id,
                        "label": m.get("name", "Unknown"),
                        "type": list(m.labels)[0] if m.labels else "Entity"
                    }

                # è¾¹
                edges.append({
                    "source": n.element_id,
                    "target": m.element_id,
                    "label": r.type
                })

            return {
                "nodes": list(nodes.values()),
                "edges": edges
            }

    # ==================== ChromaDBå‘é‡å­˜å‚¨ ====================

    def save_chunks_to_chromadb(self, chunks: List[Dict[str, Any]], doc_id: str):
        """å°†æ–‡æœ¬å—ä¿å­˜åˆ°ChromaDB"""

        # ä½¿ç”¨OpenAI Embedding APIï¼ˆDeepSeekä¹Ÿæ”¯æŒï¼‰
        texts = [chunk["content"] for chunk in chunks]

        # ç”Ÿæˆembeddingsï¼ˆæ‰¹é‡ï¼‰
        try:
            embeddings_response = self.embed_client.embeddings.create(
            model=EMBED_MODEL,
            input=texts,
            encoding_format="float" # å¯¹åº”é­”æ­çš„è¦æ±‚
        )

            embeddings = [item.embedding for item in embeddings_response.data]

            # ä¿å­˜åˆ°ChromaDB
            ids = [f"{doc_id}_{chunk['index']}" for chunk in chunks]
            metadatas = [
                {
                    "doc_id": doc_id,
                    "chunk_index": chunk["index"],
                    "length": chunk["length"]
                }
                for chunk in chunks
            ]

            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas
            )

        except Exception as e:
            raise Exception(f"å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")

    def search_similar_chunks(self, query: str, n_results: int = None) -> List[Dict[str, Any]]:
        """å‘é‡æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬å—ï¼ˆå¸¦ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼‰"""

        if n_results is None:
            n_results = VECTOR_SEARCH_TOP_K

        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding_response = self.embed_client.embeddings.create(
            model=EMBED_MODEL,
            input=[query],
            encoding_format="float"
        )

            query_embedding = query_embedding_response.data[0].embedding

            # æ£€ç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )

            # æ ¼å¼åŒ–ç»“æœå¹¶è¿‡æ»¤ç›¸ä¼¼åº¦
            chunks = []
            if results and results["documents"]:
                for i in range(len(results["documents"][0])):
                    distance = results["distances"][0][i] if "distances" in results else None

                    # ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼šåªä¿ç•™è·ç¦»å°äºé˜ˆå€¼çš„æ–‡æ¡£
                    if distance is not None and distance < VECTOR_SIMILARITY_THRESHOLD:
                        chunks.append({
                            "content": results["documents"][0][i],
                            "metadata": results["metadatas"][0][i],
                            "distance": distance
                        })

            return chunks

        except Exception as e:
            print(f"å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}")
            return []

    def search_graph_neighbors(self, entities: List[str], hops: int = None) -> Dict[str, Any]:
            """å›¾æ£€ç´¢ï¼šæŸ¥æ‰¾å®ä½“çš„é‚»å±…èŠ‚ç‚¹ï¼ˆå·²ä¿®æ­£å˜é•¿è·¯å¾„è¯­æ³•é”™è¯¯ï¼‰"""
            if not entities:
                return {"nodes": [], "edges": []}

            if hops is None:
                hops = GRAPH_SEARCH_HOPS

            with self.neo4j_driver.session() as session:
                # ä¿®æ­£åçš„æŸ¥è¯¢ï¼šä½¿ç”¨ relationships(p) è·å–è·¯å¾„ä¸­çš„æ‰€æœ‰å…³ç³»å¹¶è¿‡æ»¤
                query = f"""
                MATCH p = (n)-[*1..{hops}]-(m)
                WHERE n.name IN $entities
                AND ALL(rel IN relationships(p) WHERE type(rel) <> 'FROM_DOCUMENT')
                RETURN n, relationships(p) as r_list, m
                LIMIT 50
                """

                result = session.run(query, entities=entities)

                nodes = {}
                edges = []

                for record in result:
                    n = record["n"]
                    m = record["m"]
                    r_list = record["r_list"] # è¿™æ˜¯ä¸€ä¸ªå…³ç³»åˆ—è¡¨

                    # 1. å¤„ç†èŠ‚ç‚¹ n
                    if n.element_id not in nodes:
                        nodes[n.element_id] = {
                            "id": str(n.element_id),
                            "label": str(n.get("name", "Unknown")),
                            "type": str(list(n.labels)[0]) if n.labels else "Entity",
                            "highlighted": True
                        }

                    # 2. å¤„ç†èŠ‚ç‚¹ m
                    if m.element_id not in nodes:
                        nodes[m.element_id] = {
                            "id": str(m.element_id),
                            "label": str(m.get("name", "Unknown")),
                            "type": str(list(m.labels)[0]) if m.labels else "Entity",
                            "highlighted": True
                        }

                    # 3. å¤„ç†è·¯å¾„ä¸­çš„æ‰€æœ‰è¾¹ (ä¿®æ­£ç‚¹ï¼šå˜é•¿è·¯å¾„éœ€è¦å¾ªç¯å¤„ç†è¾¹)
                    for rel in r_list:
                        edge_id = f"{rel.start_node.element_id}-{rel.type}-{rel.end_node.element_id}"
                        # é¿å…é‡å¤æ·»åŠ è¾¹
                        if not any(e['id'] == edge_id for e in edges):
                            edges.append({
                                "id": edge_id,
                                "source": str(rel.start_node.element_id),
                                "target": str(rel.end_node.element_id),
                                "label": str(rel.type)
                            })

                return {
                    "nodes": list(nodes.values()),
                    "edges": edges
                }

    def extract_entities_from_question(self, question: str) -> List[str]:
        """ä»é—®é¢˜ä¸­æå–å®ä½“ï¼ˆç”¨äºå›¾æ£€ç´¢ï¼‰"""

        prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–å…³é”®å®ä½“åç§°ï¼Œä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ã€‚

é—®é¢˜ï¼š{question}

åªè¿”å›å®ä½“åç§°æ•°ç»„ï¼Œä¾‹å¦‚ï¼š["å¼ ä¸‰", "é˜¿é‡Œå·´å·´"]

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼š"""

        try:
            response = self.extraction_client.chat.completions.create(
                model=EXTRACTION_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå®ä½“è¯†åˆ«åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=200
            )

            result_text = response.choices[0].message.content.strip()

            # è§£æJSON
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()

            entities = json.loads(result_text)
            return entities if isinstance(entities, list) else []

        except Exception as e:
            print(f"å®ä½“æå–å¤±è´¥: {str(e)}")
            return []

    # ==================== RAGé—®ç­” ====================

    async def answer_question(self, question: str, stream: bool = False, model_name: Optional[str] = None) -> Dict[str, Any]:
        """RAGé—®ç­”ï¼ˆæ··åˆæ£€ç´¢ + LLMç”Ÿæˆï¼‰"""

        # è·å–æŒ‡å®šçš„QAå®¢æˆ·ç«¯å’Œæ¨¡å‹
        qa_client, qa_model = self.get_qa_client(model_name)

        # 1. å‘é‡æ£€ç´¢ï¼ˆä¼šè‡ªåŠ¨åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼‰
        vector_chunks = self.search_similar_chunks(question, n_results=5)

        # 2. å›¾æ£€ç´¢
        entities = self.extract_entities_from_question(question)
        graph_data = self.search_graph_neighbors(entities, hops=2)

        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []

        # åªæœ‰å½“æœ‰ç›¸ä¼¼æ–‡æ¡£æ—¶æ‰æ·»åŠ æ–‡æ¡£ç‰‡æ®µä¸Šä¸‹æ–‡
        if vector_chunks:
            context_parts.append("ã€ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‘")
            for i, chunk in enumerate(vector_chunks, 1):
                context_parts.append(f"{i}. {chunk['content']}")

        # åªæœ‰å½“æœ‰å›¾è°±èŠ‚ç‚¹æ—¶æ‰æ·»åŠ å›¾è°±ä¸Šä¸‹æ–‡
        if graph_data["nodes"]:
            context_parts.append("\nã€ç›¸å…³çŸ¥è¯†å›¾è°±ã€‘")
            for node in graph_data["nodes"][:10]:
                context_parts.append(f"- {node['label']} ({node['type']})")

        # æ„å»ºæç¤ºè¯
        if not context_parts:
            prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚æ³¨æ„ï¼šæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£æˆ–çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†ç›´æ¥å›ç­”ã€‚

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†çš„ç­”æ¡ˆï¼š"""
        else:
            context = "\n".join(context_parts)
            prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†çš„ç­”æ¡ˆï¼š"""

        try:
            if stream:
                # æµå¼è¿”å›
                response = qa_client.chat.completions.create(
                    model=qa_model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1000,
                    stream=True
                )

                return {
                    "stream": response,
                    "sources": {
                        "vector_chunks": vector_chunks,
                        "graph_data": graph_data
                    }
                }
            else:
                # éæµå¼è¿”å›
                response = qa_client.chat.completions.create(
                    model=qa_model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1000
                )

                answer = response.choices[0].message.content

                return {
                    "answer": answer,
                    "sources": {
                        "vector_chunks": vector_chunks,
                        "graph_data": graph_data
                    }
                }

        except Exception as e:
            raise Exception(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")

    async def answer_question_parallel_stream(self, question: str, conversation_id: Optional[str] = None, model_name: Optional[str] = None):
        """
        å¹¶è¡Œæµå¼RAGé—®ç­”
        ä¸‰ä¸ªéƒ¨åˆ†å¹¶è¡Œå¤„ç†ï¼šå‘é‡æ£€ç´¢ã€å›¾æ£€ç´¢ã€ç­”æ¡ˆç”Ÿæˆ
        æ¯ä¸ªéƒ¨åˆ†å®Œæˆåç«‹å³é€šè¿‡ç”Ÿæˆå™¨yieldè¿”å›

        Args:
            question: å½“å‰é—®é¢˜
            conversation_id: å¯¹è¯IDï¼Œç”¨äºä»Redisè·å–å¯¹è¯å†å²
            model_name: ä½¿ç”¨çš„AIé—®ç­”æ¨¡å‹åç§°
        """
        import json
        from redis_utils import get_conversation_history, save_conversation_message

        # è·å–æŒ‡å®šçš„QAå®¢æˆ·ç«¯å’Œæ¨¡å‹
        qa_client, qa_model = self.get_qa_client(model_name)

        # ç”¨äºå­˜å‚¨æ£€ç´¢ç»“æœï¼Œä¾›ç­”æ¡ˆç”Ÿæˆä½¿ç”¨
        vector_chunks = []
        graph_data = {"nodes": [], "edges": []}

        # ç”¨äºç´¯ç§¯å®Œæ•´ç­”æ¡ˆï¼Œä»¥ä¾¿ä¿å­˜åˆ°Redis
        full_answer = ""

        # ä»Redisè·å–å¯¹è¯å†å²
        history = []
        if conversation_id:
            history = get_conversation_history(conversation_id)
            print(f"ğŸ“š ä»Redisè·å–åˆ° {len(history)} æ¡å†å²æ¶ˆæ¯")

        # å®šä¹‰ä¸‰ä¸ªå¼‚æ­¥ä»»åŠ¡
        async def vector_search_task():
            """å‘é‡æ£€ç´¢ä»»åŠ¡"""
            try:
                loop = asyncio.get_event_loop()
                chunks = await loop.run_in_executor(
                    self.executor,
                    self.search_similar_chunks,
                    question,
                    5  # å¢åŠ æ£€ç´¢æ•°é‡ï¼Œå› ä¸ºä¼šè¢«ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                )
                vector_chunks.extend(chunks)
                return {
                    "type": "vector_chunks",
                    "data": chunks
                }
            except Exception as e:
                print(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
                return {
                    "type": "vector_chunks",
                    "data": [],
                    "error": str(e)
                }

        async def graph_search_task():
            """å›¾æ£€ç´¢ä»»åŠ¡"""
            try:
                loop = asyncio.get_event_loop()
                # æå–å®ä½“
                entities = await loop.run_in_executor(
                    self.executor,
                    self.extract_entities_from_question,
                    question
                )
                # å›¾æ£€ç´¢
                graph = await loop.run_in_executor(
                    self.executor,
                    self.search_graph_neighbors,
                    entities,
                    2
                )
                graph_data.update(graph)
                return {
                    "type": "graph_data",
                    "data": graph
                }
            except Exception as e:
                print(f"å›¾æ£€ç´¢å¤±è´¥: {e}")
                return {
                    "type": "graph_data",
                    "data": {"nodes": [], "edges": []},
                    "error": str(e)
                }

        async def answer_generation_task():
            """ç­”æ¡ˆç”Ÿæˆä»»åŠ¡ï¼ˆç­‰å¾…æ£€ç´¢å®Œæˆåå†å¼€å§‹ï¼‰"""
            nonlocal full_answer

            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©æ£€ç´¢ä»»åŠ¡å…ˆå®Œæˆ
            await asyncio.sleep(0.5)

            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []

            # åªæœ‰å½“æœ‰ç›¸ä¼¼æ–‡æ¡£æ—¶æ‰æ·»åŠ æ–‡æ¡£ç‰‡æ®µä¸Šä¸‹æ–‡
            if vector_chunks:
                context_parts.append("ã€ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‘")
                for i, chunk in enumerate(vector_chunks, 1):
                    # å¯é€‰ï¼šæ˜¾ç¤ºç›¸ä¼¼åº¦ä¿¡æ¯
                    distance = chunk.get('distance', None)
                    distance_info = f" (ç›¸ä¼¼åº¦è·ç¦»: {distance:.3f})" if distance is not None else ""
                    context_parts.append(f"{i}. {chunk['content']}{distance_info}")

            # åªæœ‰å½“æœ‰å›¾è°±èŠ‚ç‚¹æ—¶æ‰æ·»åŠ å›¾è°±ä¸Šä¸‹æ–‡
            if graph_data.get("nodes"):
                context_parts.append("\nã€ç›¸å…³çŸ¥è¯†å›¾è°±ã€‘")
                for node in graph_data["nodes"][:10]:
                    context_parts.append(f"- {node['label']} ({node['type']})")

            # å¦‚æœæ²¡æœ‰ä»»ä½•ä¸Šä¸‹æ–‡ï¼Œæç¤ºç”¨æˆ·
            if not context_parts:
                prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚æ³¨æ„ï¼šæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£æˆ–çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†ç›´æ¥å›ç­”ã€‚

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†çš„ç­”æ¡ˆï¼š"""
            else:
                context = "\n".join(context_parts)
                prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†çš„ç­”æ¡ˆï¼š"""

            try:
                # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å†å²å¯¹è¯
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹å¹¶åŸºäºä¸Šä¸‹æ–‡è¿›è¡Œå›ç­”ã€‚"}
                ]

                # æ·»åŠ å†å²å¯¹è¯ï¼ˆé™åˆ¶æœ€è¿‘çš„5è½®å¯¹è¯ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
                if history:
                    # åªä¿ç•™æœ€è¿‘çš„5è½®å¯¹è¯ï¼ˆ10æ¡æ¶ˆæ¯ï¼š5ä¸ªé—®é¢˜ + 5ä¸ªå›ç­”ï¼‰
                    recent_history = history[-10:] if len(history) > 10 else history
                    for msg in recent_history:
                        if msg.get("role") and msg.get("content"):
                            messages.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })

                # æ·»åŠ å½“å‰é—®é¢˜ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
                messages.append({
                    "role": "user",
                    "content": prompt
                })

                # æµå¼ç”Ÿæˆç­”æ¡ˆ
                response = qa_client.chat.completions.create(
                    model=qa_model,
                    messages=messages,
                    temperature=0.3,
                    max_tokens=1000,
                    stream=True
                )

                return {
                    "type": "answer_stream",
                    "stream": response
                }
            except Exception as e:
                print(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
                return {
                    "type": "answer_stream",
                    "error": str(e)
                }

        # å¹¶å‘æ‰§è¡Œæ£€ç´¢ä»»åŠ¡
        search_tasks = [vector_search_task(), graph_search_task()]

        # ä½¿ç”¨ as_completed æ¥è·å–å…ˆå®Œæˆçš„ä»»åŠ¡
        for coro in asyncio.as_completed(search_tasks):
            result = await coro
            # ç«‹å³è¿”å›æ£€ç´¢ç»“æœ
            yield json.dumps(result, ensure_ascii=False) + "\n"

        # ç­‰å¾…æ£€ç´¢å…¨éƒ¨å®Œæˆåï¼Œå¼€å§‹ç­”æ¡ˆç”Ÿæˆ
        answer_result = await answer_generation_task()

        if "stream" in answer_result:
            # æµå¼è¿”å›ç­”æ¡ˆ
            for chunk in answer_result["stream"]:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_answer += content  # ç´¯ç§¯å®Œæ•´ç­”æ¡ˆ

                    answer_chunk = {
                        "type": "answer",
                        "content": content
                    }
                    yield json.dumps(answer_chunk, ensure_ascii=False) + "\n"

            # ç­”æ¡ˆç»“æŸæ ‡è®°
            yield json.dumps({"type": "answer_done"}, ensure_ascii=False) + "\n"

            # ä¿å­˜å¯¹è¯åˆ°Redis
            if conversation_id:
                # ä¿å­˜ç”¨æˆ·é—®é¢˜
                save_conversation_message(conversation_id, "user", question)
                # ä¿å­˜AIå›ç­”
                save_conversation_message(conversation_id, "assistant", full_answer)
                print(f"ğŸ’¾ å¯¹è¯å·²ä¿å­˜åˆ°Redis: {conversation_id}")
        else:
            # å¦‚æœç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œè¿”å›é”™è¯¯
            yield json.dumps(answer_result, ensure_ascii=False) + "\n"


# å…¨å±€æœåŠ¡å®ä¾‹
kg_service = KnowledgeGraphService()
