"""
Spark éŸ³ä¹æ‰¹å¤„ç†ä½œä¸š
ä» Kafka è¯»å–æ’­æ”¾äº‹ä»¶ï¼Œç”Ÿæˆæ—¥æ¦œ/å‘¨æ¦œ/æœˆæ¦œï¼Œåˆ†æç”¨æˆ·åå¥½
"""

import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, count, sum as spark_sum, 
    window, desc, rank, when, lit
)
from pyspark.sql.window import Window
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, LongType
)

from neo4j_writer import (
    write_daily_rankings,
    write_weekly_rankings,
    write_monthly_rankings,
    write_user_preferences,
    update_total_play_stats
)
from kafka_offset_utils import get_kafka_consumer, get_offsets_by_timestamp

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®å‚æ•°
KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'
MUSIC_PLAY_EVENTS_TOPIC = 'music-play-events'

# æ’­æ”¾äº‹ä»¶ Schema
EVENT_SCHEMA = StructType([
    StructField("event_id", StringType(), True),
    StructField("song_id", IntegerType(), True),
    StructField("user_id", StringType(), True),
    StructField("timestamp", LongType(), True),
    StructField("event_type", StringType(), True)
])


def create_spark_session(app_name: str = "MusicAnalytics"):
    """åˆ›å»º Spark Sessionï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰

    Args:
        app_name: åº”ç”¨åç§°

    Returns:
        SparkSession: Spark ä¼šè¯å¯¹è±¡
    """
    spark = SparkSession.builder \
        .appName(app_name) \
        .master("local[2]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1") \
        .getOrCreate()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    spark.sparkContext.setLogLevel("WARN")
    
    logger.info(f"âœ… Spark Session åˆ›å»ºæˆåŠŸ: {app_name}")
    return spark


def read_kafka_events(spark: SparkSession, start_timestamp: int, end_timestamp: int):
    """ä» Kafka è¯»å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ’­æ”¾äº‹ä»¶ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„ offset æŸ¥è¯¢ï¼‰

    âœ… æ€§èƒ½ä¼˜åŒ–è¯´æ˜:
    æœ¬å‡½æ•°ä½¿ç”¨ Kafka Consumer API å…ˆæŸ¥è¯¢ç²¾ç¡®çš„ offset èŒƒå›´ï¼Œç„¶åè®© Spark åªè¯»å–æŒ‡å®šèŒƒå›´å†…çš„æ•°æ®ã€‚
    è¿™æ ·å¯ä»¥é¿å…è¯»å–å¤§é‡å†å²æ•°æ®ï¼Œæ˜¾è‘—æå‡æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å½“ Kafka topic ä¿ç•™æ—¶é—´é•¿ï¼ˆ7å¤©+ï¼‰æ—¶ã€‚

    Args:
        spark: Spark Session
        start_timestamp: å¼€å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        end_timestamp: ç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰

    Returns:
        DataFrame: æ’­æ”¾äº‹ä»¶æ•°æ®
    """
    consumer = None
    try:
        # 1. ä½¿ç”¨ Kafka Consumer API æŸ¥è¯¢ç²¾ç¡®çš„ offset èŒƒå›´
        consumer = get_kafka_consumer()
        
        offset_info = get_offsets_by_timestamp(
            consumer,
            MUSIC_PLAY_EVENTS_TOPIC,
            start_timestamp,
            end_timestamp
        )
        
        # 2. ä» Kafka è¯»å–æ•°æ®ï¼ˆä½¿ç”¨ç²¾ç¡®çš„ offset èŒƒå›´ï¼‰
        df = spark.read \
            .format("kafka") \
            .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
            .option("subscribe", MUSIC_PLAY_EVENTS_TOPIC) \
            .option("startingOffsets", offset_info['startingOffsets']) \
            .option("endingOffsets", offset_info['endingOffsets']) \
            .load()
        
        # è§£æ JSON å€¼
        event_df = df.select(
            from_json(col("value").cast("string"), EVENT_SCHEMA).alias("event")
        ).select("event.*")
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´
        filtered_df = event_df.filter(
            (col("timestamp") >= start_timestamp) & 
            (col("timestamp") < end_timestamp)
        )
        
        count = filtered_df.count()
        logger.info(f"âœ… ä» Kafka è¯»å–äº‹ä»¶: æ—¶é—´èŒƒå›´=[{start_timestamp}, {end_timestamp}), å®é™…è¯»å–={count}æ¡")
        
        return filtered_df
        
    except Exception as e:
        logger.error(f"âŒ ä» Kafka è¯»å–äº‹ä»¶å¤±è´¥: {e}")
        raise
    
    finally:
        # 3. å…³é—­ Kafka Consumer
        if consumer:
            consumer.close()


def calculate_growth_rate(current_count: int, previous_count: int) -> float:
    """è®¡ç®—ç¯æ¯”å¢é•¿ç‡

    Args:
        current_count: å½“å‰æ’­æ”¾æ¬¡æ•°
        previous_count: ä¸Šä¸€æ¬¡æ’­æ”¾æ¬¡æ•°

    Returns:
        float: å¢é•¿ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    """
    if previous_count == 0:
        return 100.0 if current_count > 0 else 0.0
    return ((current_count - previous_count) / previous_count) * 100


def run_daily_stats(date: str):
    """æ‰§è¡Œæ¯æ—¥ç»Ÿè®¡ä½œä¸š

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼: '2026-01-04'
    """
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥ç»Ÿè®¡ä½œä¸š: date={date}")
    
    spark = None
    try:
        # 1. åˆ›å»º Spark Session
        spark = create_spark_session("DailyStats")
        
        # 2. è®¡ç®—æ—¶é—´èŒƒå›´
        date_dt = datetime.strptime(date, '%Y-%m-%d')
        start_timestamp = int(date_dt.timestamp() * 1000)
        end_timestamp = int((date_dt + timedelta(days=1)).timestamp() * 1000)
        
        # 3. è¯»å– Kafka äº‹ä»¶
        event_df = read_kafka_events(spark, start_timestamp, end_timestamp)
        
        if event_df.count() == 0:
            logger.warning(f"âš ï¸ è¯¥æ—¥æœŸæ²¡æœ‰æ’­æ”¾äº‹ä»¶: date={date}")
            return
        
        # 4. æŒ‰æ­Œæ›²èšåˆç»Ÿè®¡æ’­æ”¾æ¬¡æ•°
        song_stats = event_df.groupBy("song_id").agg(
            count("*").alias("play_count")
        ).orderBy(desc("play_count"))
        
        # 5. è®¡ç®—æ’å
        window_spec = Window.orderBy(desc("play_count"))
        song_stats = song_stats.withColumn("rank", rank().over(window_spec))
        
        # 6. è·å–å‰ä¸€å¤©çš„æ’­æ”¾æ¬¡æ•°ï¼ˆè®¡ç®—å¢é•¿ç‡ï¼‰
        prev_date_dt = date_dt - timedelta(days=1)
        prev_date = prev_date_dt.strftime('%Y-%m-%d')
        
        # ä» Neo4j è¯»å–å‰ä¸€å¤©çš„æ•°æ®
        from database import driver
        prev_stats = {}
        
        try:
            with driver.session() as session:
                query = """
                    MATCH (s:Song)
                    WHERE s.daily_rank_date = date($date)
                    RETURN s.id as song_id, s.daily_play_count as play_count
                """
                result = session.run(query, date=prev_date)
                for record in result:
                    prev_stats[record["song_id"]] = record["play_count"]
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–å‰ä¸€å¤©æ•°æ®å¤±è´¥: {e}")
        
        # 7. æ”¶é›†ç»“æœå¹¶è®¡ç®—å¢é•¿ç‡
        rankings = []
        for row in song_stats.collect():
            song_id = row["song_id"]
            play_count = row["play_count"]
            rank = row["rank"]
            prev_count = prev_stats.get(song_id, 0)
            growth_rate = calculate_growth_rate(play_count, prev_count)
            
            rankings.append({
                "song_id": song_id,
                "rank": rank,
                "play_count": play_count,
                "growth_rate": round(growth_rate, 2)
            })
        
        # 8. å†™å…¥ Neo4j
        write_daily_rankings(rankings, date)
        
        # 9. æ›´æ–°æ•´ä½“ç»Ÿè®¡
        update_total_play_stats(date)
        
        logger.info(f"âœ… æ¯æ—¥ç»Ÿè®¡ä½œä¸šå®Œæˆ: date={date}, total_songs={len(rankings)}")
        
    except Exception as e:
        logger.error(f"âŒ æ¯æ—¥ç»Ÿè®¡ä½œä¸šå¤±è´¥: date={date}, {e}")
        raise
    finally:
        if spark:
            spark.stop()
            logger.info("ğŸ”’ Spark Session å·²å…³é—­")


def run_weekly_stats(week: str):
    """æ‰§è¡Œæ¯å‘¨ç»Ÿè®¡ä½œä¸š

    Args:
        week: å‘¨å­—ç¬¦ä¸²ï¼Œæ ¼å¼: '2026-W01'ï¼ˆISO å‘¨æ ¼å¼ï¼‰
    """
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ¯å‘¨ç»Ÿè®¡ä½œä¸š: week={week}")
    
    spark = None
    try:
        # 1. åˆ›å»º Spark Session
        spark = create_spark_session("WeeklyStats")
        
        # 2. è§£æå‘¨èŒƒå›´
        year, week_num = week.split('-W')
        year = int(year)
        week_num = int(week_num)
        
        # è®¡ç®—å‘¨çš„èµ·æ­¢æ—¶é—´
        # ISO å‘¨çš„ç¬¬ä¸€å¤©æ˜¯å‘¨ä¸€
        start_date = datetime.fromisocalendar(year, week_num, 1)
        end_date = start_date + timedelta(days=7)
        
        start_timestamp = int(start_date.timestamp() * 1000)
        end_timestamp = int(end_date.timestamp() * 1000)
        
        # 3. è¯»å– Kafka äº‹ä»¶
        event_df = read_kafka_events(spark, start_timestamp, end_timestamp)
        
        if event_df.count() == 0:
            logger.warning(f"âš ï¸ è¯¥å‘¨æ²¡æœ‰æ’­æ”¾äº‹ä»¶: week={week}")
            return
        
        # 4. æŒ‰æ­Œæ›²èšåˆç»Ÿè®¡æ’­æ”¾æ¬¡æ•°
        song_stats = event_df.groupBy("song_id").agg(
            count("*").alias("play_count")
        ).orderBy(desc("play_count"))
        
        # 5. è®¡ç®—æ’å
        window_spec = Window.orderBy(desc("play_count"))
        song_stats = song_stats.withColumn("rank", rank().over(window_spec))
        
        # 6. æ”¶é›†ç»“æœ
        rankings = []
        for row in song_stats.collect():
            rankings.append({
                "song_id": row["song_id"],
                "rank": row["rank"],
                "play_count": row["play_count"]
            })
        
        # 7. å†™å…¥ Neo4j
        write_weekly_rankings(rankings, week)
        
        logger.info(f"âœ… æ¯å‘¨ç»Ÿè®¡ä½œä¸šå®Œæˆ: week={week}, total_songs={len(rankings)}")
        
    except Exception as e:
        logger.error(f"âŒ æ¯å‘¨ç»Ÿè®¡ä½œä¸šå¤±è´¥: week={week}, {e}")
        raise
    finally:
        if spark:
            spark.stop()
            logger.info("ğŸ”’ Spark Session å·²å…³é—­")


def run_monthly_stats(month: str):
    """æ‰§è¡Œæ¯æœˆç»Ÿè®¡ä½œä¸š

    Args:
        month: æœˆå­—ç¬¦ä¸²ï¼Œæ ¼å¼: '2026-01'
    """
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æœˆç»Ÿè®¡ä½œä¸š: month={month}")
    
    spark = None
    try:
        # 1. åˆ›å»º Spark Session
        spark = create_spark_session("MonthlyStats")
        
        # 2. è§£ææœˆèŒƒå›´
        year, month_num = map(int, month.split('-'))
        start_date = datetime(year, month_num, 1)
        
        # è®¡ç®—ä¸‹ä¸ªæœˆçš„ç¬¬ä¸€å¤©
        if month_num == 12:
            end_date = datetime(year + 1, 1, 1)
        else:
            end_date = datetime(year, month_num + 1, 1)
        
        start_timestamp = int(start_date.timestamp() * 1000)
        end_timestamp = int(end_date.timestamp() * 1000)
        
        # 3. è¯»å– Kafka äº‹ä»¶
        event_df = read_kafka_events(spark, start_timestamp, end_timestamp)
        
        if event_df.count() == 0:
            logger.warning(f"âš ï¸ è¯¥æœˆæ²¡æœ‰æ’­æ”¾äº‹ä»¶: month={month}")
            return
        
        # 4. æŒ‰æ­Œæ›²èšåˆç»Ÿè®¡æ’­æ”¾æ¬¡æ•°
        song_stats = event_df.groupBy("song_id").agg(
            count("*").alias("play_count")
        ).orderBy(desc("play_count"))
        
        # 5. è®¡ç®—æ’å
        window_spec = Window.orderBy(desc("play_count"))
        song_stats = song_stats.withColumn("rank", rank().over(window_spec))
        
        # 6. æ”¶é›†ç»“æœ
        rankings = []
        for row in song_stats.collect():
            rankings.append({
                "song_id": row["song_id"],
                "rank": row["rank"],
                "play_count": row["play_count"]
            })
        
        # 7. å†™å…¥ Neo4j
        write_monthly_rankings(rankings, month)
        
        logger.info(f"âœ… æ¯æœˆç»Ÿè®¡ä½œä¸šå®Œæˆ: month={month}, total_songs={len(rankings)}")
        
    except Exception as e:
        logger.error(f"âŒ æ¯æœˆç»Ÿè®¡ä½œä¸šå¤±è´¥: month={month}, {e}")
        raise
    finally:
        if spark:
            spark.stop()
            logger.info("ğŸ”’ Spark Session å·²å…³é—­")


def run_user_preference_analysis(date: str):
    """æ‰§è¡Œç”¨æˆ·åå¥½åˆ†æä½œä¸š

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼: '2026-01-04'
    """
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œç”¨æˆ·åå¥½åˆ†æä½œä¸š: date={date}")
    
    spark = None
    try:
        # 1. åˆ›å»º Spark Session
        spark = create_spark_session("UserPreferenceAnalysis")
        
        # 2. è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆåˆ†ææœ€è¿‘ 30 å¤©ï¼‰
        date_dt = datetime.strptime(date, '%Y-%m-%d')
        start_date = date_dt - timedelta(days=30)
        
        start_timestamp = int(start_date.timestamp() * 1000)
        end_timestamp = int((date_dt + timedelta(days=1)).timestamp() * 1000)
        
        # 3. è¯»å– Kafka äº‹ä»¶
        event_df = read_kafka_events(spark, start_timestamp, end_timestamp)
        
        if event_df.count() == 0:
            logger.warning(f"âš ï¸ è¯¥æ—¶é—´æ®µæ²¡æœ‰æ’­æ”¾äº‹ä»¶: date={date}")
            return
        
        # 4. æŒ‰ç”¨æˆ·èšåˆç»Ÿè®¡
        user_stats = event_df.groupBy("user_id", "song_id").agg(
            count("*").alias("play_count")
        )
        
        # 5. ä¸ºæ¯ä¸ªç”¨æˆ·è®¡ç®— Top 5 æ­Œæ›²
        user_window = Window.partitionBy("user_id").orderBy(desc("play_count"))
        user_stats = user_stats.withColumn("rank", rank().over(user_window))
        
        # 6. è¿‡æ»¤ Top 5
        top_songs = user_stats.filter(col("rank") <= 5)
        
        # 7. è®¡ç®—ç”¨æˆ·æ€»æ’­æ”¾æ¬¡æ•°
        user_total_plays = event_df.groupBy("user_id").agg(
            count("*").alias("total_plays")
        )
        
        # 8. æŒ‰ç”¨æˆ·æ”¶é›†ç»“æœ
        user_preference_dict = {}
        for row in user_total_plays.collect():
            user_id = row["user_id"]
            user_preference_dict[user_id] = {
                "username": user_id,
                "total_plays": row["total_plays"],
                "last_active_at": date,
                "top_songs": [],
                "genre_preferences": {}
            }
        
        # 9. å¡«å…… Top 5 æ­Œæ›²
        for row in top_songs.collect():
            user_id = row["user_id"]
            user_preference_dict[user_id]["top_songs"].append({
                "song_id": row["song_id"],
                "play_count": row["play_count"]
            })
        
        # TODO: å®ç°éŸ³ä¹ç±»å‹åå¥½åˆ†æï¼ˆéœ€è¦ä» Neo4j è·å–æ­Œæ›²çš„ genre ä¿¡æ¯ï¼‰
        # è¿™é‡Œæš‚æ—¶ç•™ç©ºï¼Œåç»­å¯ä»¥æ‰©å±•
        
        # 10. å†™å…¥ Neo4jï¼ˆæ‰¹å¤„ç†ï¼‰
        processed_count = 0
        for user_id, preference in user_preference_dict.items():
            try:
                write_user_preferences(preference, date)
                processed_count += 1
            except Exception as e:
                logger.error(f"âŒ å†™å…¥ç”¨æˆ·åå¥½å¤±è´¥: user_id={user_id}, {e}")
        
        logger.info(f"âœ… ç”¨æˆ·åå¥½åˆ†æä½œä¸šå®Œæˆ: date={date}, processed_users={processed_count}")
        
    except Exception as e:
        logger.error(f"âŒ ç”¨æˆ·åå¥½åˆ†æä½œä¸šå¤±è´¥: date={date}, {e}")
        raise
    finally:
        if spark:
            spark.stop()
            logger.info("ğŸ”’ Spark Session å·²å…³é—­")


if __name__ == "__main__":
    import sys
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    if len(sys.argv) < 3:
        print("Usage: python spark_music_analytics.py <daily|weekly|monthly|user-preference> <date|week|month>")
        print("Example:")
        print("  python spark_music_analytics.py daily 2026-01-04")
        print("  python spark_music_analytics.py weekly 2026-W01")
        print("  python spark_music_analytics.py monthly 2026-01")
        print("  python spark_music_analytics.py user-preference 2026-01-04")
        sys.exit(1)
    
    task_type = sys.argv[1].lower()
    time_param = sys.argv[2]
    
    try:
        if task_type == "daily":
            run_daily_stats(time_param)
        elif task_type == "weekly":
            run_weekly_stats(time_param)
        elif task_type == "monthly":
            run_monthly_stats(time_param)
        elif task_type == "user-preference":
            run_user_preference_analysis(time_param)
        else:
            logger.error(f"âŒ æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
            sys.exit(1)
            
        logger.info("âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
